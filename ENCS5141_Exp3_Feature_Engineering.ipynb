{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "FvUqiEq48jzK",
        "jbyES8XIB7zS",
        "q9UpJC3vAlR7"
      ],
      "authorship_tag": "ABX9TyPlky/run75JbRFYKRx+Oa3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkjubran/ENCS5141-INTELLIGENT-SYSTEMS-LAB/blob/main/ENCS5141_Exp3_Feature_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EXPLORATORY DATA ANALYSIS â€“ FEATURE SELECTION AND FEATURE ENGINEERING"
      ],
      "metadata": {
        "id": "mkRHIvM06yZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Experiment #3: Feature Selection and Feature Engineering\n",
        "\n",
        "This experiment focuses on discussing concepts and implementing code snippets to demonstrate various techniques used for feature selection nd feature engineering. These steps are vital in preparing data for machine learning and analysis. Throughout the experiment, you will also work on exercises to apply your knowledge and develop essential skills. The topics that will be discussed in the experiment are\n",
        "\n",
        "##3.1 Feature Selection\n",
        "3.1.1 Filtering Method using Variance \\\n",
        "3.1.2 Filtering Method using Chi-squared Test \\\n",
        "3.1.3 Filtering Method using Correlation Coefficient \\\n",
        "3.1.4 Filtering Method using Information Gain \\\n",
        "\n",
        "##3.2 Data Transformation\n",
        "3.2.1 Scalling \\\n",
        "3.3.2 Discreateization \\\n",
        "3.3.3 Encoding \\\n",
        "\n",
        "##3.3 Handling High-Dimensional Data\n",
        "3.3.1 Principal Component Analysis \\\n",
        "3.3.2 Linear Discriminant Analysis \\\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "pFB412fkroLI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.1 Feature Selection\n"
      ],
      "metadata": {
        "id": "0og2RNis1mWU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature selection is a critical data preprocessing technique in machine learning that involves choosing a subset of the most relevant features (variables or columns) from a dataset. The goal of feature selection is to improve the performance of machine learning models by reducing the dimensionality of the data, eliminating irrelevant or redundant features, and enhancing model interpretability and generalization. There are several common feature selection methods, which can be categorized into three main types: filter methods, wrapper methods, and embedded methods.\n",
        "*   **Filter Methods** involves using statistical measures to identify and select the most relevant features from a dataset. Some common filter methods for feature selection include:\n",
        "\n",
        "  - **Variance**: This method removes features that have a low variance, which means that they do not vary much across the data. This can help to remove noise and irrelevant features.\n",
        "  - **Chi-squared test**: This method is used to test the independence of two variables. It can be used to identify features that are correlated with the target variable, which can be useful for classification and regression tasks.\n",
        "  - **Correlation coefficient**: This method measures the linear relationship between two variables. It can be used to identify features that are highly correlated with each other, which can be removed to reduce multicollinearity.\n",
        "  - **Information gain**: This method measures the reduction in entropy that is achieved by knowing the value of a feature. It can be used to identify features that are most informative for predicting the target variable.\n",
        "- **Wrapper Methods** are techniques that involve selecting the optimal subset of features by repeatedly training and evaluating the machine learning model with different feature subsets. These methods are called \"wrapper\" methods because they wrap around the machine learning algorithm to find the best feature set based on model performance.Wrapper methods are model-specific, meaning they consider the performance of a specific machine learning algorithm. This can lead to highly relevant feature subsets for that particular algorithm.\n",
        "- **Embedded Methods** are techniques that incorporate feature selection directly into the process of training a machine learning model. Unlike filter methods that evaluate feature relevance independently of the chosen model, and wrapper methods that involve a separate evaluation process, embedded methods select features during the model training process itself. Some common examples of embedded methods are:\n",
        "  - **L1 Regularization (Lasso)**: Penalizes the absolute values of feature coefficients, effectively encouraging sparse feature sets.\n",
        "  - **Tree-Based Methods**: Decision trees and ensemble methods like Random Forest and Gradient Boosting inherently provide feature importance scores based on node impurity or information gain.\n",
        "  - **Linear Support Vector Machines (SVM)**: SVMs can automatically select relevant support vectors, effectively reducing the dimensionality.\n",
        "\n",
        " In this experiment, our primary emphasis will be on filtering methods, while we will explore wrapper and embedded methods in upcoming experiments."
      ],
      "metadata": {
        "id": "cV8bWUF1KrZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1.1 Filtering Method using Variance\n",
        "To demonstrate the filtering method using the variance threshold, let us start with synthetic data as presented in the code snippet below."
      ],
      "metadata": {
        "id": "c2JirLv0psC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import VarianceThreshold\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Sample dataset with low-variance features\n",
        "data = [[1.0, 2.4, 3.01],\n",
        "     [1.1, 2.2, 3.04],\n",
        "     [1.2, 2.1, 3.04],\n",
        "     [1.1, 2.6, 3.06],\n",
        "     [1.0, 2.5, 3.00],\n",
        "     [1.1, 2.2, 3.01]]\n",
        "\n",
        "columns =['col_1', 'col_2', 'col_3']\n",
        "\n",
        "df = pd.DataFrame(data=data, columns=columns)\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "-1p86a0lqGCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To assess the variance of individual features and apply the variance-based filtering method, we will employ the **VarianceThreshol()** method from sklearn. This method calculates the variance for each column (feature) and eliminates features with variances below a specified threshold, which we will set to 0 initially, thus retaining all features."
      ],
      "metadata": {
        "id": "CKFAyGeyqsA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "# Initialize the VarianceThreshold selector with a threshold\n",
        "selector = VarianceThreshold(threshold=0)\n",
        "\n",
        "# Fit the selector to the data\n",
        "selector = selector.fit(df)\n",
        "\n",
        "# Apply the selector to the data\n",
        "df_high_variance = selector.transform(df)\n",
        "\n",
        "# Variance of the data\n",
        "selector.variances_"
      ],
      "metadata": {
        "id": "F8nyaDvUqyYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's evident that the 'col_3' feature has significantly lower variance in comparison to the other features, and it will be excluded if we establish a threshold value of 0.001."
      ],
      "metadata": {
        "id": "alz2PQdAtOMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the VarianceThreshold selector with a threshold\n",
        "selector = VarianceThreshold(threshold=0.001)\n",
        "\n",
        "# Fit and apply the selector to the data\n",
        "df_high_variance = selector.fit_transform(df)\n",
        "\n",
        "# Display the original and selected data\n",
        "print(\"Original Data:\")\n",
        "print(df)\n",
        "\n",
        "print(\"Data after VarianceThreshold:\")\n",
        "print(df_high_variance)"
      ],
      "metadata": {
        "id": "fLpCQBaVtl9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3.1**: To understand the variance of a feature, modify the values of both feature **col_3** and feature **col_1** in such a way that, in the previous code snippet with a **0.001** threshold, the **col_3 feature will be retained, while the col_1 feature will be eliminated**.\n"
      ],
      "metadata": {
        "id": "Ot_zVH23lMpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#write you code here\n"
      ],
      "metadata": {
        "id": "_M91SU8AmZFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To demonstrate the affect of applying the variance threshold filtering method on machine learning, we will implement this approach on the iris dataset. The Iris dataset is a classic and widely used dataset in machine learning and statistics. Within this dataset, there are 150 instances, each featuring 4 continuous, numeric attributes. The target variable, categorized by class, signifies the different Iris species: Iris-setosa, Iris-versicolor, and Iris-virginica. The attributes, also referred to as features, encompass the following dimensions:\n",
        "- Sepal Length (measured in centimeters): This dimension quantifies the length of the sepal, which is the outermost part of the flower.\n",
        "- Sepal Width (measured in centimeters): This dimension specifies the width of the sepal.\n",
        "- Petal Length (measured in centimeters): This dimension characterizes the length of the petal, which constitutes the inner part of the flower.\n",
        "- Petal Width (measured in centimeters): This dimension quantifies the width of the petal.\n",
        "\n",
        "To display images of Iris flowers, run the following code.\n"
      ],
      "metadata": {
        "id": "VaLFJLHS_bUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.display import Image, display\n",
        "display(Image('http://mirlab.org/jang/books/dcpr/image/Iris-setosa-10_1.jpg', width=200, height=200))\n",
        "print (\"Iris Setosa\\n\")\n",
        "\n",
        "display(Image('http://mirlab.org/jang/books/dcpr/image/Iris-versicolor-21_1.jpg', width=200, height=200))\n",
        "print (\"Iris Versicolor\\n\")\n",
        "\n",
        "display(Image('http://mirlab.org/jang/books/dcpr/image/Iris-virginica-3_1.jpg', width=200, height=200))\n",
        "print (\"Iris Virginica\")"
      ],
      "metadata": {
        "id": "TwoTc6a8BbFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To import the dataset and display the features of a sample, use the following code."
      ],
      "metadata": {
        "id": "zxi8ehIBCm5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "import numpy as np\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Create a DataFrame to work with the dataset\n",
        "iris_df = pd.DataFrame(data=np.c_[X, y], columns=data.feature_names + ['target'])\n",
        "\n",
        "iris_df.head()"
      ],
      "metadata": {
        "id": "fCvRonkwAP2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As previously explained, the dataset comprises four features and a label. To obtain the variance of each feature run the following code snippet."
      ],
      "metadata": {
        "id": "96sJLL95nIEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Variance Threshold\n",
        "selector = VarianceThreshold(threshold=0.3) # the threshold value is set to 0.3\n",
        "X_variance = selector.fit_transform(X)\n",
        "\n",
        "print(f\"The variance of each featue: {selector.variances_}\")"
      ],
      "metadata": {
        "id": "t2cC8Pv1mxRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As observed, the features have varying variance values, and therefore, the feature selection method can be used to eliminate low variance features.\n",
        "\n",
        "To investigate the impact of the variance threshold filtering method on machine learning, we will compare the performance of the machine learning models with and without feature filtering. In this context, we will employ the **train_test_split()** method to partition the dataset into training and testing datasets. Subsequently, we will utilize the training data to train the random forest classifier. Following the training, we will assess both training and testing accuracies (classification correctness). More comprehensive details regarding the random forest classifier and the training and evaluation processes will be discussed in future experiments."
      ],
      "metadata": {
        "id": "jNsWy4Asx37g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a classifier on the original features and evaluate\n",
        "clf = RandomForestClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Apply Variance Threshold\n",
        "selector = VarianceThreshold(threshold=0.3)\n",
        "X_train_variance = selector.fit_transform(X_train)\n",
        "X_test_variance = selector.transform(X_test)\n",
        "\n",
        "# Train a classifier on the selected features and evaluate\n",
        "clf = RandomForestClassifier(random_state=42)\n",
        "clf.fit(X_train_variance, y_train)\n",
        "y_pred = clf.predict(X_test_variance)\n",
        "accuracy_variance = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"The variance of each featue: {selector.variances_}\")\n",
        "print(f\"Number of original features: {X_train.shape[1]}\")\n",
        "print(f\"Number of features after variance threshold filtering: {X_train_variance.shape[1]}\")\n",
        "print(f\"Accuracy of Original features (testing accuracy): {accuracy}\")\n",
        "print(f\"Accuracy after variance threshold filtering (testing accuracy): {accuracy_variance}\")\n"
      ],
      "metadata": {
        "id": "0Prog92XvwPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the order of applying feature filtering (feature selection or feature engineering) concerning splitting the dataset into training and testing sets is an important consideration in machine learning workflows. Here are the common approaches:\n",
        "- **Apply Feature Filtering Before Splitting**:\\\n",
        "  *Advantages*:\n",
        "  - Filtering is performed on the entire dataset, ensuring that the selected features are consistent across training and testing sets.\n",
        "  - It can help reduce the risk of data leakage, where information from the testing set inadvertently influences the feature selection process.\n",
        "\n",
        "  *Considerations*:\n",
        "  - Make sure to perform feature selection using only the training data and then apply the same feature selection to the testing data to maintain consistency.\n",
        "\n",
        "- **Apply Feature Filtering After Splitting**: \\\n",
        "  *Advantages*:\n",
        "  - Feature filtering is applied separately to the training and testing sets, which mimics a real-world scenario where you may not have access to the entire dataset during feature selection.\n",
        "  - Reduces the risk of overfitting to the training data during feature selection.\n",
        "\n",
        "  *Considerations*:\n",
        "  - Ensure that the same feature selection criteria or method is applied consistently to both the training and testing sets to maintain fairness in evaluation.\n",
        "\n",
        "The choice between these approaches depends on factors such as the dataset size, the nature of the features, and the risk of data leakage. In practice, both methods can be effective if implemented correctly. In the preceding code snippet, feature filtering is applied prior to dataset splitting."
      ],
      "metadata": {
        "id": "Uexl2Sr7sU7D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3.2**: To comprehend the effect of feature filtering using variance threshold on machine learning models, alter the value of the feature selection threshold in the previous code snippet and observe the number of eliminated and retained features, as well as the model's performance before and after feature filtering."
      ],
      "metadata": {
        "id": "_tngznDGv0Le"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1.2 Filtering Method using Chi-squared Test\n",
        "To demonstrate the filtering method using the Chi-squared test, we will apply the **SelectKBest()** method with the **Chi2** function from sklearn to the iris dataset."
      ],
      "metadata": {
        "id": "FvUqiEq48jzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a classifier on the original features and evaluate\n",
        "clf = RandomForestClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Apply Chi2 test selector\n",
        "selector = SelectKBest(chi2,k=2)\n",
        "X_train_Chi2 = selector.fit_transform(X_train,y_train)\n",
        "X_test_Chi2 = selector.transform(X_test)\n",
        "\n",
        "# Train a classifier on the selected features and evaluate\n",
        "clf = RandomForestClassifier(random_state=42)\n",
        "clf.fit(X_train_Chi2, y_train)\n",
        "y_pred = clf.predict(X_test_Chi2)\n",
        "accuracy_Chi2 = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"The Chi2 score of each featue: {selector.scores_}\")\n",
        "print(f\"Number of original features: {X_train.shape[1]}\")\n",
        "print(f\"Number of features after Chi2 test filtering: {X_train_variance.shape[1]}\")\n",
        "print(f\"Accuracy of Original features (testing accuracy): {accuracy}\")\n",
        "print(f\"Accuracy after Chi2 test filtering (testing accuracy): {accuracy_Chi2}\")\n"
      ],
      "metadata": {
        "id": "pZ8cgIPA-TKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3.3**: To comprehend the effect of feature filtering using Chi-squared test on machine learning models, alter the number of selected features in the previous code snippet and observe the number of eliminated and retained features, as well as the model's performance before and after feature filtering."
      ],
      "metadata": {
        "id": "cZ2zigaFsO8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1.3 Filtering Method using Correlation Coefficient"
      ],
      "metadata": {
        "id": "jbyES8XIB7zS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3.4**: Modify the code snippet in the 'Filtering Method using Chi-squared Test' section to demonstrate feature filtering using Correlation Coefficient. To calculate correlation coefficient, utilize the **r_regression** function in sklearn."
      ],
      "metadata": {
        "id": "pbjLv1q_EqJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#write you code here\n"
      ],
      "metadata": {
        "id": "2vrQ0tTZEq_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1.4 Filtering Method using Information Gain"
      ],
      "metadata": {
        "id": "cjY7feYeCLfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3.5**: Modify the code snippet in the 'Filtering Method using Chi-squared Test' section to demonstrate feature filtering using Information Gain. To calculate information gain, utilize the **mutual_info_classif** function in sklearn."
      ],
      "metadata": {
        "id": "yrW_j5NMEuj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#write you code here\n"
      ],
      "metadata": {
        "id": "WcveSfAzEvRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Filtering Methods **Task 3.6**\n",
        "In this task, you will compare different feature selection techniques. You will apply these techniques to a modified version of the Pima Indians Diabetes Database from Kaggle (https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database). This dataset is originally from the Indian Institute of Diabetes and Digestive and Kidney Diseases. The dataset's objective is to predict, based on certain diagnostic measurements, whether a patient has diabetes or not. The Diabetes dataset is stored in a file named **ENCS5141_Exp3_Diabetes.csv**, which can be found in the GitHub repository located at https://github.com/mkjubran/ENCS5141Datasets.\n",
        "\n",
        "After cloning the repository using the code below, apply the following procedure to solve the task:\n",
        "\n",
        "1. Read the dataset from the ENCS5141_Exp3_Diabetes.csv file.\n",
        "2. Apply the Random Forest classifier to the original dataset and measure the classification (testing) accuracy.\n",
        "3. Utilize the four feature filtering methods mentioned previously for feature selection, then apply the Random Forest classifier to the selected features, and measure the testing accuracies for each method.\n",
        "4. Experiment with varying the parameters of each feature filtering method and observe the effect on the classification accuracy of the Random Forest classifier."
      ],
      "metadata": {
        "id": "wfBshSnFE4ri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ./ENCS5141Datasets\n",
        "!git clone https://github.com/mkjubran/ENCS5141Datasets.git"
      ],
      "metadata": {
        "id": "s134_jeCG8Sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#write you code here\n"
      ],
      "metadata": {
        "id": "U_JpY7a3E7c4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#3.2 Data Transformation\n"
      ],
      "metadata": {
        "id": "UzrzNdSYKiqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Transformation is a crucial step in the data preprocessing phase of machine learning. It involves a series of operations aimed at preparing raw data into a format that is suitable for training and evaluating machine learning models. Data transformation encompasses several key processes, including scaling, normalization, discretization, and encoding, each serving its purpose in enhancing the quality and usability of the data. Here's a description of these data transformation techniques:\n",
        "\n",
        "- **Scaling (Normalization)**: This method involves transforming numerical features to a common scale without changing their relative relationships. This is particularly important for algorithms that are sensitive to the magnitude of features. Scaling is applied to ensure that no single feature dominates the learning process due to its larger magnitude. The Min-Max scaling is a commonly used method in which features are scaled to a specific range, often [0, 1]. The standardization is another type of scaling that transforms numerical features to have a a zero mean and unity variance (mean=0, standard deviation=1).\n",
        "\n",
        "- **Discretization**: This method involves converting continuous numerical data into discrete categories or bins. This is particularly useful when you want to transform numeric data into categorical or ordinal data. Discretization can simplify complex numerical data and make it suitable for algorithms that work with categorical or ordinal features. Some common techniques include equal-width binning (dividing the data into equal-width intervals) and equal-frequency binning (ensuring each bin contains approximately the same number of data points).\n",
        "\n",
        "- **Encoding**: Encoding is the process of converting categorical data (text or labels) into a numerical format that machine learning models can understand. It allows algorithms to work with non-numeric data Some common encoding methods include **one-hot encoding** (creating binary columns for each category), **label encoding** (assigning a unique integer to each category), and **ordinal encoding** (mapping ordinal categories to numerical values)."
      ],
      "metadata": {
        "id": "fPwH-Cv7LijH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2.1 Scaling"
      ],
      "metadata": {
        "id": "cSjUJUxTOkY1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To demonstrate the Scaling method, let us start with synthetic data as presented in the code snippet below."
      ],
      "metadata": {
        "id": "kOxE-CruoGm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#To generate an array of floating numbers between -2 and 2\n",
        "A = np.random.random(1000)*4 - 2\n",
        "\n",
        "#To generate an array of floating numbers between -10 and 10\n",
        "B = np.random.random(1000)*20 - 10\n",
        "\n",
        "#To generate an array of floating numbers between 0 and 1\n",
        "C = np.random.random(1000)\n",
        "\n",
        "#To generate an array of floating numbers between 0 and 10\n",
        "D = np.random.random(1000)*10\n",
        "\n",
        "df = pd.DataFrame({'A':A,'B':B,'C':C,'D':D})\n",
        "df.head()"
      ],
      "metadata": {
        "id": "8TK6gRX5OvlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also display some statistics about this synthetic data."
      ],
      "metadata": {
        "id": "xFoLXxSSbJ8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "66NXIgM4bKhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.2.1.1 MinMaxScaler**\n",
        "\n",
        "As can be observed, the range of values for each feature is different. To scale the data, we will use the **MinMaxScaler()** from sklearn. This method takes the intended minimum and maximum values of the scaled data as a tuple argument (min, max), which is set to (0, 1) by default."
      ],
      "metadata": {
        "id": "5A-wlhH3awVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(df)\n",
        "data = scaler.transform(df)\n",
        "df_scaler = pd.DataFrame(data,columns=list('ABCD'))\n",
        "df_scaler.describe()"
      ],
      "metadata": {
        "id": "FoSCR9URa-bS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After scaling, all features are scaled between 0 and 1. Let's plot the distribution of the original and scaled data."
      ],
      "metadata": {
        "id": "2MXlEvdcfNtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "fig, axs = plt.subplots(figsize=(18, 5),ncols=4,nrows=2)\n",
        "\n",
        "sns.histplot(data=df, x=\"A\", ax=axs[0,0], kde=True);axs[0,0].set_title(f\"Original A\")\n",
        "sns.histplot(data=df, x=\"B\", ax=axs[0,1], kde=True);axs[0,1].set_title(f\"Original B\")\n",
        "sns.histplot(data=df, x=\"C\", ax=axs[0,2], kde=True);axs[0,2].set_title(f\"Original C\")\n",
        "sns.histplot(data=df, x=\"D\", ax=axs[0,3], kde=True);axs[0,3].set_title(f\"Original D\")\n",
        "\n",
        "sns.histplot(data=df_scaler, x=\"A\", ax=axs[1,0], kde=True);axs[1,0].set_title(f\"Scalled A\")\n",
        "sns.histplot(data=df_scaler, x=\"B\", ax=axs[1,1], kde=True);axs[1,1].set_title(f\"Scalled B\")\n",
        "sns.histplot(data=df_scaler, x=\"C\", ax=axs[1,2], kde=True);axs[1,2].set_title(f\"Scalled C\")\n",
        "sns.histplot(data=df_scaler, x=\"D\", ax=axs[1,3], kde=True);axs[1,3].set_title(f\"Scalled D\")\n",
        "\n",
        "fig.subplots_adjust(hspace=1)"
      ],
      "metadata": {
        "id": "nFk0vmeFgPxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The histogram plots illustrate that scaling did not alter the distribution of the features but did affect the range of their values."
      ],
      "metadata": {
        "id": "XB2qyC4AhTaz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To illustrate the significance of feature scaling in many machine learning scenarios, consider the data within the **ENCS5141_Exp3_Mall_Customers.csv** file, available in the GitHub repository at https://github.com/mkjubran/ENCS5141Datasets. This dataset includes the annual income and spending score of numerous mall customers. As before, we will initiate by cloning the GitHub repository. You may skip this step if the repository has already been cloned"
      ],
      "metadata": {
        "id": "jwRHE-UIlj4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ./ENCS5141Datasets\n",
        "!git clone https://github.com/mkjubran/ENCS5141Datasets.git"
      ],
      "metadata": {
        "id": "NUFmR5O5okNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To read the file and display information about the features run the following code"
      ],
      "metadata": {
        "id": "I-Iw5rNpoozk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/ENCS5141Datasets/ENCS5141_Exp3_Mall_Customers.csv\", index_col=0)\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "_IQozrgrowk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be observed, the Annual Income feature has a range in the thousands, whereas the Spending Score is in tens."
      ],
      "metadata": {
        "id": "XQKtW0oNqIys"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To investigate the impact of the scalling on machine learning, we will compare the performance of the machine learning models with and without feature scalling. In this context, we will use the K-Means classifier to cluster the data into groups. The classifier will be discussed in detail in subsequent experimental sessions."
      ],
      "metadata": {
        "id": "yFGd2yjMiCl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the KMeans class from the sklearn.cluster module\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Create a KMeans object with the desired number of clusters (4 in this case)\n",
        "km = KMeans(n_clusters=4)\n",
        "\n",
        "# Fit the KMeans model to the data, using the 'Annual Income' and 'Spending Score' columns as features\n",
        "km.fit(df[['Annual Income', 'Spending Score']])\n",
        "\n",
        "# Predict the cluster labels for each data point based on the fitted model\n",
        "cluster = km.predict(df[['Annual Income', 'Spending Score']])\n",
        "\n",
        "# Create a new column 'cluster' in the DataFrame to store the predicted cluster labels\n",
        "df['cluster'] = cluster\n",
        "\n",
        "# Display the first few rows of the DataFrame with the 'cluster' column added\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Hwhj8OiitH5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To visualize the resuls, use the **sns.scatterplot()** as below"
      ],
      "metadata": {
        "id": "Cc7xc411wHJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the seaborn library for data visualization\n",
        "import seaborn as sns\n",
        "\n",
        "# Create a scatter plot:\n",
        "# - x-axis: 'Annual Income'\n",
        "# - y-axis: 'Spending Score'\n",
        "# - 'hue' parameter assigns different colors to data points based on the 'cluster' column\n",
        "# - 'style' parameter assigns different markers/styles to data points based on the 'cluster' column\n",
        "# - 'size' parameter adjusts the size of data points based on the 'cluster' column\n",
        "# - 'palette' parameter defines the color palette used for the plot\n",
        "sns.scatterplot(x=df['Annual Income'], y=df['Spending Score'], hue=df['cluster'], style=df['cluster'], size=df['cluster'], palette='colorblind')"
      ],
      "metadata": {
        "id": "YcucRtS7taen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the scatter plot, it's evident that the points are grouped or clustered based on the 'Annual Income' (with values in thousands) and do not take into account the 'Spending Score' feature (with values in tens). Is this an acceptable clustering approach?"
      ],
      "metadata": {
        "id": "thqB5VlAws86"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us repeate the classification but after feature scalling."
      ],
      "metadata": {
        "id": "Uj4EViFmxS_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the MinMaxScaler class from the sklearn.preprocessing module\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Import the KMeans class from the sklearn.cluster module\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Create a MinMaxScaler object\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit the scaler to the DataFrame 'df', which computes the minimum and maximum values for scaling\n",
        "scaler.fit(df)\n",
        "\n",
        "# Transform and scale the data in 'df' using the fitted scaler\n",
        "data = scaler.transform(df)\n",
        "\n",
        "# Create a new DataFrame 'df_scaler' to store the scaled data, maintaining column names from 'df'\n",
        "df_scaler = pd.DataFrame(data, columns=df.columns)\n",
        "\n",
        "\n",
        "# Create a KMeans object with the desired number of clusters (4 in this case)\n",
        "km = KMeans(n_clusters=4)\n",
        "\n",
        "# Fit the KMeans model to the data, using the 'Annual Income' and 'Spending Score' columns as features\n",
        "km.fit(df_scaler[['Annual Income', 'Spending Score']])\n",
        "\n",
        "# Predict the cluster labels for each data point based on the fitted model\n",
        "cluster = km.predict(df_scaler[['Annual Income', 'Spending Score']])\n",
        "\n",
        "# Create a new column 'cluster' in the DataFrame to store the predicted cluster labels\n",
        "df_scaler['cluster'] = cluster\n",
        "\n",
        "# Create a scatter plot:\n",
        "# - x-axis: 'Annual Income'\n",
        "# - y-axis: 'Spending Score'\n",
        "# - 'hue' parameter assigns different colors to data points based on the 'cluster' column\n",
        "# - 'style' parameter assigns different markers/styles to data points based on the 'cluster' column\n",
        "# - 'size' parameter adjusts the size of data points based on the 'cluster' column\n",
        "# - 'palette' parameter defines the color palette used for the plot\n",
        "sns.scatterplot(x=df_scaler['Annual Income'], y=df_scaler['Spending Score'], hue=df_scaler['cluster'], style=df_scaler['cluster'], size=df_scaler['cluster'], palette='colorblind')\n"
      ],
      "metadata": {
        "id": "LjMQcxOgxZSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3.7**: Referring to the figure shown above, please provide your observations regarding the clustering of Mall Customers' records both before and after the scaling process."
      ],
      "metadata": {
        "id": "HM13dnExyIao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#write you code here\n"
      ],
      "metadata": {
        "id": "AQy0t8FOy9mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3.8**: To demonstrate the impact of scaling on machine learning, your task is to cluster the records in the 'tips' dataset based on the **total_bill** and **tip** features, both before and after applying feature scaling.\n",
        "\n",
        "**Note**: The tips dataset contains information about tips received by a waiter over a few months in a restaurant. It has details like how much tip was given, the bill amount, whether the person paying the bill is male or female, if there were smokers in the group, the day of the week, the time of day, and the size of the group.\n",
        "\n",
        "You may load the dataset using the following code snippet."
      ],
      "metadata": {
        "id": "3U5PJn1fzGn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load an example dataset\n",
        "tips = sns.load_dataset(\"tips\")\n",
        "tips.info()"
      ],
      "metadata": {
        "id": "d-gQxE9IzP-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#write you code here\n"
      ],
      "metadata": {
        "id": "5pAQIKHh1dY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.2.1.2 Standardization**\n",
        "\n",
        "**Task 3.9**: To explore the standardization method of scaling, employ the **StandardScaler()** method in sklearn to standardize the synthetic data generated in the code snippet below:\n",
        "1. Display the description of the synthetic data.\n",
        "2. Utilize StandardScaler() to standardize the data and display the description of the standardized data.\n",
        "3. Observe the mean and standard deviation of the original and standardized data.\n",
        "4. Plot the distribution of both the original and standardized data."
      ],
      "metadata": {
        "id": "FvjE6vnJ1qd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Generation of Synthetic Data\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#To generate an array of floating numbers between -2 and 2\n",
        "A = np.random.random(1000)*4 - 2\n",
        "\n",
        "#To generate an array of floating numbers between -10 and 10\n",
        "B = np.random.random(1000)*20 - 10\n",
        "\n",
        "#To generate an array of floating numbers between 0 and 1\n",
        "C = np.random.random(1000)\n",
        "\n",
        "#To generate an array of floating numbers between 0 and 10\n",
        "D = np.random.random(1000)*10\n",
        "\n",
        "df = pd.DataFrame({'A':A,'B':B,'C':C,'D':D})\n",
        "df.head()"
      ],
      "metadata": {
        "id": "z3fJSbrz10Iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#write you code here\n"
      ],
      "metadata": {
        "id": "E9NlJV9bAk2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2.2 Discretization\n",
        "\n"
      ],
      "metadata": {
        "id": "q9UpJC3vAlR7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To demonstrate discretization, we will load the \"fetch_california_housing\" dataset. This dataset is a popular dataset used in machine learning. It contains data related to housing in California, primarily for the purpose of regression analysis. This dataset is often used for tasks such as predicting the median house value for districts in California based on various features. The dataset includes several features that can be used as input variables for regression models. Some of the typical features include median income, average house occupancy, latitude, longitude, and more. The target variable is the median house value for each district. This is the variable that you typically aim to predict in regression tasks."
      ],
      "metadata": {
        "id": "DcpnZqMuoL5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "X,y = fetch_california_housing(return_X_y= True, as_frame = True)\n",
        "df = pd.concat((X,y), axis=1)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "dQNvN-tSF4Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.2.2.1 Equal Width Discretiser**\n",
        "\n",
        "Equal Width Discretiser is a data preprocessing technique used in feature engineering to transform continuous numerical data into discrete categories of equal width. This method divides the range of the data into a specified number of bins or intervals, ensuring that each bin has the same width or range."
      ],
      "metadata": {
        "id": "QV2lKeQzbTUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the feature_engine library if not already installed\n",
        "!pip install feature_engine\n",
        "\n",
        "# Import the required library and module\n",
        "from feature_engine import discretisation as dsc\n",
        "\n",
        "# Set up the discretisation transformer with specified parameters:\n",
        "# - 'bins' sets the number of bins or intervals (8 in this case)\n",
        "# - 'variables' specifies the columns to be discretized\n",
        "# - if 'return_boundaries=True' is set, then it indicates that you want to return bin boundaries and not the discreate values\n",
        "disc = dsc.EqualWidthDiscretiser(bins=8, variables=['MedInc'])\n",
        "\n",
        "# Fit the transformer on the selected variables\n",
        "disc.fit(df[['MedInc']])\n",
        "\n",
        "# Create a copy of the original DataFrame to store the discretized data\n",
        "df_EqualWidthDiscretiser = df.copy()\n",
        "\n",
        "# Transform the specified columns using the fitted discretisation transformer\n",
        "df_EqualWidthDiscretiser[['MedInc']] = disc.transform(df[['MedInc']])\n",
        "\n",
        "# Access the bin boundaries dictionary (not required for the main discretization)\n",
        "disc.binner_dict_"
      ],
      "metadata": {
        "id": "_4dDAc4WYiQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To print the DataFrame after discretisation of the MedInc"
      ],
      "metadata": {
        "id": "3lG_sR6siSbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_EqualWidthDiscretiser.head()"
      ],
      "metadata": {
        "id": "wa2l58oehCvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be observed, the 'MedInc' feature values have been converted into discrete values. To print the width of the bins used for discretization, execute the following code:"
      ],
      "metadata": {
        "id": "MfxRrtZSc1EO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(disc.binner_dict_['MedInc'][1:]) - np.array(disc.binner_dict_['MedInc'][0:-1])"
      ],
      "metadata": {
        "id": "x7XAKINSdAMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As observed, all bins, except the first and last ones, have the same width. Let us plot the distribution before and after discretisation."
      ],
      "metadata": {
        "id": "EJTs0gMFdBjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "fig, axs = plt.subplots(figsize=(18, 5),ncols=2,nrows=1)\n",
        "\n",
        "sns.histplot(data=df, x=\"MedInc\", ax=axs[0], kde=True);axs[0].set_title(f\"Original\")\n",
        "sns.histplot(data=df_EqualWidthDiscretiser, x=\"MedInc\", ax=axs[1], kde=True);axs[1].set_title(f\"After Equal Width Discretiser\")\n",
        "fig.subplots_adjust(hspace=1)"
      ],
      "metadata": {
        "id": "UgPMdkHndi8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3.9**: What do you observe from the distributions of the MedInc feature before and after discretisation?"
      ],
      "metadata": {
        "id": "HIk0SvZKfKY0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.2.2.2 Equal-frequency discretization**\n",
        "\n",
        "Equal-frequency discretization is another data preprocessing technique used in feature engineering to transform continuous numerical data into discrete categories of equal frequency. This method divides the range of the data into a specified number of bins or intervals, ensuring that each bin contains approximately the same number of data points.\n",
        "\n",
        "**Task 3.10**: In this task, you will utilize the **KBinsDiscretizer** from scikit-learn (sklearn.preprocessing) to:\n",
        "1. Fit and apply the KBinsDiscretizer to the 'MedInc' feature in the 'fetch_california_housing' dataset.\n",
        "2. Plot and observe the histogram for the transformed feature."
      ],
      "metadata": {
        "id": "Dwj6WdbjjwOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#write you code here\n"
      ],
      "metadata": {
        "id": "ZSFS2OODmsbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.2.3 Encoding\n"
      ],
      "metadata": {
        "id": "QGAziDZFnFDi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding involves converting categorical data, such as text or labels, into a numerical format that machine learning models can interpret. In this experiment, you will investigate three encoding methods: one-hot encoding (creating binary columns for each category), label encoding (assigning a unique integer to each category), and ordinal encoding (mapping ordinal categories to numerical values)."
      ],
      "metadata": {
        "id": "l0ASVGVxoP0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.2.3.1 One-hot encoding**\n",
        "\n",
        "To demonstrate one-hot encoding, You will apply these techniques to a modified version of the Medical Cost Personal Dataset from Kaggle (https://www.kaggle.com/datasets/mirichoi0218/insurance). This dataset provides information pertaining to healthcare and medical insurance costs. It includes the following features:\n",
        "\n",
        "1. Age: Represents the age of the insured individuals.\n",
        "2. Gender: Indicates the gender of the insured individuals (male or female).\n",
        "3. BMI (Body Mass Index): A numerical measure that assesses body weight in relation to height.\n",
        "4. Children: Denotes the number of children or dependents covered by the insurance plan.\n",
        "5. Smoker: Specifies whether the insured individuals are smokers (with values typically as \"yes\" or \"no\").\n",
        "6. Region: Describes the geographic region or location where the insured individuals reside.\n",
        "7. Charges: Represents the medical insurance charges or costs incurred by the insured individuals.\n",
        "\n",
        "This dataset is stored in a file named **ENCS5141_Exp3_MedicalCostPersonalDatasets.csv**, which can be found in the GitHub repository located at https://github.com/mkjubran/ENCS5141Datasets.\n",
        "\n",
        "To clone the repository if you haven't already done so, execute the following code"
      ],
      "metadata": {
        "id": "r8QnbIWGptLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ./ENCS5141Datasets\n",
        "!git clone https://github.com/mkjubran/ENCS5141Datasets.git"
      ],
      "metadata": {
        "id": "zSa7Cu1_sihf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To read the file into a DataFrame and display information about the file run the following code snippet"
      ],
      "metadata": {
        "id": "Oym3BIiUud7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/ENCS5141Datasets/ENCS5141_Exp3_MedicalCostPersonalDatasets.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "kER00es1upfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the **OneHotEncoder** from sklearn to encode the **gender** feature."
      ],
      "metadata": {
        "id": "lz28xJf-u2Ag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary library for one-hot encoding\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Create an instance of the OneHotEncoder with 'handle_unknown' set to 'ignore'\n",
        "enc = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "# Fit the encoder on the 'gender' column of the DataFrame\n",
        "enc.fit(df[['gender']])\n",
        "\n",
        "# Transform the 'gender' column into a one-hot encoded array and convert it to a dense array\n",
        "df_gender = enc.transform(df[['gender']]).toarray()\n",
        "\n",
        "# Create a copy of the original DataFrame to store the one-hot encoded data\n",
        "df_ohenc = df.copy()\n",
        "\n",
        "# Add the one-hot encoded columns to the new DataFrame using the encoder's categories\n",
        "df_ohenc[enc.categories_[0]] = df_gender\n",
        "\n",
        "# Display the first few rows of the new DataFrame\n",
        "df_ohenc.head()\n"
      ],
      "metadata": {
        "id": "7lVSOg5UvIx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each category is represented by its own distinct binary column in the encoding. Remember to exclude the original 'gender' (text) feature from the encoded dataset before using it for machine learning purposes"
      ],
      "metadata": {
        "id": "aP8llgXAz098"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3.11**: use the OneHotEncoder from sklearn from sklearn to encode the **smoker** and **region** features."
      ],
      "metadata": {
        "id": "GBzSD--H0Inv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#write you code here\n"
      ],
      "metadata": {
        "id": "lUXLaq7h0_n-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.2.3.1 Label encoding**\n",
        "\n",
        "**Task 3.12**: use the LabelEncoder from sklearn (sklearn.preprocessing) to encode all text features in the Medical Cost Personal Dataset and observe the difference between one-hot encoding and label encoding."
      ],
      "metadata": {
        "id": "son0bKTj1AVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#write you code here\n"
      ],
      "metadata": {
        "id": "gLOBZBjP15HI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.3 Handling High-Dimensional Data"
      ],
      "metadata": {
        "id": "2zW8jLW3K3Ee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To tackle this challenge, various techniques and strategies are employed. In this experiment, we will specifically investigate dimensionality reduction techniques as an approach to address high-dimensional data. Our focus will be on two prominent methods for dimensionality reduction: Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA).\n",
        "\n",
        "**Principal Component Analysis (PCA)**: PCA is one of the most widely used techniques for reducing the dimensionality of data. It transforms the original features into a new set of orthogonal features called principal components. These components capture the maximum variance in the data, allowing for dimensionality reduction while retaining as much information as possible.\n",
        "\n",
        "**Linear Discriminant Analysis (LDA)**: LDA is used when the goal is not just dimensionality reduction but also class separation. It finds a linear combination of features that maximizes the distance between different classes while minimizing the variance within each class.\n",
        "\n",
        "Both PCA and LDA are dimensionality reduction techniques, PCA is unsupervised and aims to capture maximum variance, whereas LDA is supervised and focuses on maximizing class separation. The choice between PCA and LDA depends on the specific goals of your machine learning task and whether you are dealing with a classification problem."
      ],
      "metadata": {
        "id": "SoOaOpxm3SeD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.3.1 Principle component Analysis"
      ],
      "metadata": {
        "id": "X5gFmSW24UOp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**3.1.1.1 Synthetic Data**\n",
        "\n",
        "To demonstrate the PCA, let us start with synthetic data as presented in the code snippet below."
      ],
      "metadata": {
        "id": "067wTTCv4Keu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#To generate an array of floating numbers between -2 and 2\n",
        "A = np.random.random(1000)*4 - 2\n",
        "\n",
        "#To generate an array of floating numbers between -10 and 10\n",
        "B = np.random.random(1000)*20 - 10\n",
        "\n",
        "#To generate an array of floating numbers between 0 and 1\n",
        "C = np.random.random(1000)\n",
        "\n",
        "#To generate an array of floating numbers between 0 and 10\n",
        "D = np.random.random(1000)*10\n",
        "\n",
        "df = pd.DataFrame({'A':A,'B':B,'C':C,'D':D})\n",
        "df.head()"
      ],
      "metadata": {
        "id": "fZ8U_wP75Sdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilize scikit-learn's **PCA** (Principal Component Analysis) to derive PCA components from the synthetic data. In this context, we will configure the n_components parameter to yield a result of 4 components."
      ],
      "metadata": {
        "id": "GCPBqrDJ7v36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the PCA module from scikit-learn\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Create an instance of PCA with n_components set to 4\n",
        "pca = PCA(n_components=4)\n",
        "\n",
        "# Fit the PCA model on the data (assuming 'df' contains your dataset)\n",
        "pca.fit(df)\n",
        "\n",
        "# Print the explained variance ratio for each selected component\n",
        "print(f\"Explained variance ratio for each PCA component are {pca.explained_variance_ratio_}\")\n",
        "\n",
        "# Transform the original DataFrame 'df' using PCA\n",
        "Array_PCA = pca.transform(df)\n",
        "\n",
        "# Create a new DataFrame 'df_PCA' from the transformed data\n",
        "df_PCA = pd.DataFrame(Array_PCA)\n",
        "\n",
        "# Display the first few rows of the new DataFrame\n",
        "df_PCA.head()"
      ],
      "metadata": {
        "id": "sSUmGcMi5iQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observing the explained variance ratio, which indicates how much of the total variance in the original dataset is accounted for by each principal component, we notice a decreasing trend. This suggests that the initial PCA components have higher variance compared to the later ones. Now, let's visualize the histograms before and after applying PCA"
      ],
      "metadata": {
        "id": "dOtilkL78Ejb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "fig, axs = plt.subplots(figsize=(18, 5),ncols=4,nrows=2)\n",
        "\n",
        "sns.histplot(data=df, x=\"A\", ax=axs[0,0], kde=True);axs[0,0].set_title(f\"Original A\")\n",
        "sns.histplot(data=df, x=\"B\", ax=axs[0,1], kde=True);axs[0,1].set_title(f\"Original B\")\n",
        "sns.histplot(data=df, x=\"C\", ax=axs[0,2], kde=True);axs[0,2].set_title(f\"Original C\")\n",
        "sns.histplot(data=df, x=\"D\", ax=axs[0,3], kde=True);axs[0,3].set_title(f\"Original D\")\n",
        "\n",
        "sns.histplot(data=df_PCA, x=0, ax=axs[1,0], kde=True);axs[1,0].set_title(f\"PCA 1'st Component\")\n",
        "sns.histplot(data=df_PCA, x=1, ax=axs[1,1], kde=True);axs[1,1].set_title(f\"PCA 2'nd Component\")\n",
        "sns.histplot(data=df_PCA, x=2, ax=axs[1,2], kde=True);axs[1,2].set_title(f\"PCA 3'rd Component\")\n",
        "sns.histplot(data=df_PCA, x=3, ax=axs[1,3], kde=True);axs[1,3].set_title(f\"PCA 4'th Component\")\n",
        "\n",
        "fig.subplots_adjust(hspace=1)"
      ],
      "metadata": {
        "id": "k_ns36zC6qJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the distribution of features in the original data differs from that of the PCA components. Additionally, observe the range of values in the PCA components, where the first component covers a wider range than the second, and this pattern continues for the other PCA components."
      ],
      "metadata": {
        "id": "AmqQ3JdL-b6C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Task 3.13**: Modify the numer of components to be preserved by PCA in the previous code and examine the outcomes."
      ],
      "metadata": {
        "id": "NN_X416m-8bq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1.1.2 \"Digits\" Dataset**\n",
        "\n",
        "To assess the influence of PCA on machine learning, we will evaluate and compare the performance of machine learning models with and without employing PCA for feature processing. We will load the \"Digits\" dataset from sklearn. This dataset is a commonly employed for practicing classification algorithms. Here are the key characteristics of the Digits dataset:\n",
        "- **Data Source**: The dataset consists of 8x8 pixel images of handwritten digits (0 through 9). These images are grayscale and were originally collected from different individuals.\n",
        "- **Data Format**: Each image is represented as an 8x8 matrix of pixel values. In scikit-learn, these matrices are flattened into 64-element feature vectors, where each element represents the intensity of a pixel (ranging from 0 to 16).\n",
        "- **Target Labels**: For each image, there is a corresponding label (target) that indicates the digit it represents (0 through 9). These labels are commonly used for classification tasks, where the goal is to train a machine learning model to recognize handwritten digits.\n",
        "\n",
        "In this context, we will employ the **train_test_split()** method to partition the dataset into training and testing datasets. Subsequently, we will utilize the training data to train the random forest classifier. Following the training, we will assess the testing accuracies (classification correctness) before and after PCA. More comprehensive details regarding the random forest classifier and the training and evaluation processes will be discussed in future experiments."
      ],
      "metadata": {
        "id": "KeNzqbfB_gEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Digits dataset\n",
        "digits = load_digits()\n",
        "X, y = digits.data, digits.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a classifier on the original features and evaluate\n",
        "clf = RandomForestClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Create PCA instance and fit to the data\n",
        "pca = PCA(n_components=8)  # Specify the number of components\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "# Train a classifier on the retained PCA components and evaluate\n",
        "clf = RandomForestClassifier(random_state=42)\n",
        "clf.fit(X_train_pca, y_train)\n",
        "y_pred = clf.predict(X_test_pca)\n",
        "accuracy_variance = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the explained variance ratio for each selected component\n",
        "print(f\"Explained variance ratio for each PCA component: {pca.explained_variance_ratio_}\")\n",
        "\n",
        "print(f\"Number of original features: {X_train.shape[1]}\")\n",
        "print(f\"Number of features retained after PCA: {X_train_pca.shape[1]}\")\n",
        "print(f\"Accuracy of Original features (testing accuracy): {accuracy}\")\n",
        "print(f\"Accuracy after PCA (testing accuracy): {accuracy_variance}\")"
      ],
      "metadata": {
        "id": "LuADzuIeB280"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3.14**: Adjust the number of components to be retained by PCA in the code snippet for classifying images in the Digits dataset and observe how it affects the classification accuracy."
      ],
      "metadata": {
        "id": "g0S9izvdGDVo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3.15**: In this task, you will reinforce the knowledge acquired during the experiment concerning the impact of applying PCA on machine learning. You will be working with the \"Faces in the Wild\" (LFW) dataset. The LFW dataset is a widely recognized benchmark dataset within the domains of computer vision and machine learning, primarily used for face recognition tasks. It comprises a collection of grayscale images featuring human faces, with each image associated with the identity of the depicted individual.\n",
        "\n",
        "Once you have loaded the dataset, your objective is to implement the following steps:\n",
        "1. Split the dataset into training and testing subsets.\n",
        "2. Train a random forest model using the training data.\n",
        "3. Assess the model's accuracy by evaluating its performance on the testing data.\n",
        "4. Apply PCA with a specified number of components, such as n=8.\n",
        "5. Train and evaluate the random forest model using the retained PCA components.\n",
        "6. Experiment with different numbers of retained PCA components and provide insights based on your observations.\n",
        "\n",
        "\n",
        "You can load the LFW dataset using the provided code snippet."
      ],
      "metadata": {
        "id": "Fph-du2gGfFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_lfw_people\n",
        "\n",
        "lfw_dataset = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
        "\n",
        "# Extract the data and target labels\n",
        "X = lfw_dataset.data\n",
        "y = lfw_dataset.target"
      ],
      "metadata": {
        "id": "5EUfEBkLG7tf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#write you code here\n"
      ],
      "metadata": {
        "id": "fxCcsomRJaZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3.2 Linear Discriminant Analysis"
      ],
      "metadata": {
        "id": "zTbM4LwD4ZVS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To assess the influence of Linear Discriminant Analysis (LDA) on machine learning, we will evaluate and compare the performance of machine learning models with and without employing LDA for feature processing. We will continue to work with the \"Digits\" dataset as in the previous section. We will follow the same steps as explained earlier: first, we will employ the train_test_split() method to split the dataset into training and testing subsets. Then, we will use the training data to train a random forest classifier. After training, we will evaluate both the testing accuracy (classification correctness) both before and after applying LDA."
      ],
      "metadata": {
        "id": "p_iqJm7iJz1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: In LDA, unlike PCA, you typically don't need to specify the number of components explicitly. Instead, the number of components in LDA is determined by the number of unique classes or categories minus one (C - 1), where C is the number of classes or categories in your dataset. This is because LDA aims to maximize class separation, and it's constrained by the number of classes."
      ],
      "metadata": {
        "id": "dmM2XcmVPUoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA  # Import LDA\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Digits dataset\n",
        "digits = load_digits()\n",
        "X, y = digits.data, digits.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a classifier on the original features and evaluate\n",
        "clf = RandomForestClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Create LDA instance and fit to the data\n",
        "lda = LDA()  # No need to specify the number of components\n",
        "X_train_lda = lda.fit_transform(X_train,y_train)\n",
        "X_test_lda = lda.transform(X_test)\n",
        "\n",
        "# Train a classifier on the retained PCA components and evaluate\n",
        "clf = RandomForestClassifier(random_state=42)\n",
        "clf.fit(X_train_lda, y_train)\n",
        "y_pred = clf.predict(X_test_lda)\n",
        "accuracy_variance = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the explained variance ratio for each selected component\n",
        "print(f\"Explained variance ratio for each LDA component: {lda.explained_variance_ratio_}\")\n",
        "\n",
        "print(f\"Number of original features: {X_train.shape[1]}\")\n",
        "print(f\"Number of features retained after LDA: {X_train_lda.shape[1]}\")\n",
        "print(f\"Accuracy of Original features (testing accuracy): {accuracy}\")\n",
        "print(f\"Accuracy after LDA (testing accuracy): {accuracy_variance}\")"
      ],
      "metadata": {
        "id": "WMehfUswNswv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3.16**: Repeat the procedure outlined in Task 3.15, but this time, utilize Linear Discriminant Analysis (LDA) instead of PCA."
      ],
      "metadata": {
        "id": "_ws7YN1JP3-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#write you code here\n"
      ],
      "metadata": {
        "id": "56KeBTo9P4uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Case Study 3.1**\n",
        "In this case study, you will perform essential data preprocessing steps on the Penguins dataset. The dataset contains information about different species of penguins, including their physical characteristics and the region where they were observed. Your goal is to prepare the dataset for machine learning analysis. Follow these steps:\n",
        "1. Load the penguins dataset using the code snippet provided below.\n",
        "2. Perform initial data exploration to understand the dataset's structure, features, and any missing values. Summarize the dataset's statistics and gain insights into the data.\n",
        "3. Address any data quality issues, such as missing values and outliers. Decide on an appropriate strategy for handling missing data, such as imputation or removal of rows/columns.\n",
        "4. Analyze the relevance of each feature for your machine learning task by using the learned use feature selection technques.\n",
        "5. If the dataset contains categorical variables, encode them into a numerical format suitable for machine learning models.\n",
        "6. Split the dataset into training and testing subsets to evaluate the performance of your machine learning models.\n",
        "7. Scale or normalize the numerical features to ensure consistent scaling across variables.\n",
        "8. Apply suitable dimensionality reduction techniques to reduce the size of the data while preserving important information.\n",
        "9. Validate your preprocessing pipeline by training and evaluating a machine learning model, such as the Random Forest model, on the preprocessed data. Compare the results to the model trained on the raw data (before feature filtering, transformation, and reduction) to ensure that preprocessing has improved model performance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H6uSwCdoQI-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the 'load_dataset' function from seaborn to load the penguins dataset\n",
        "from seaborn import load_dataset\n",
        "\n",
        "# Load the penguins dataset and store it in the 'df' DataFrame\n",
        "df = load_dataset('penguins')\n",
        "\n",
        "# Display the first few rows of the DataFrame to get an initial look at the data\n",
        "df.head()"
      ],
      "metadata": {
        "id": "zviyIaMwS5T5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#write you code here\n"
      ],
      "metadata": {
        "id": "8egqEOIsTAmD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}