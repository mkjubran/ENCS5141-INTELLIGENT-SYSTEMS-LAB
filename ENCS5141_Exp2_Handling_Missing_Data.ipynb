{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyNWFQnJMdubHSo/HApsK8c0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkjubran/ENCS5141-INTELLIGENT-SYSTEMS-LAB/blob/main/ENCS5141_Exp2_Handling_Missing_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Experiment #2: Data Visualization and Data Cleaning\n",
        "\n",
        "This experiment focuses on discussing concepts and implementing code snippets to demonstrate various techniques used for data cleaning as part of an Exploratory Data Analysis (EDA). EDA plays a crucial role in comprehending and examining datasets. Throughout the experiment, you will also need to solve a few exercises to demonstrate your comprehension and acquire the necessary skills. The topics that will be discussed in the experiment are\n",
        "##2.1 Data Visualization\n",
        "2.1.1 Using Matplotlib \\\n",
        "2.1.2 Using Seaborn \\\n",
        "2.1.3 Using Pandas \\\n",
        "2.1.4 Boxplot \\\n",
        "##2.2 Descriptive statistics\n",
        "2.2.1 Central Tendency \\\n",
        "2.2.2 Variation \\\n",
        "2.2.3 Shape of Distribution \\\n",
        "2.2.4 Quantiles \\\n",
        "##2.3 Handling Missing Data\n",
        "2.3.1 Missing Numeric Data \\\n",
        "2.3.2 Missing Categorical Data \\\n",
        "##2.4 Handling Outliers\n",
        "2.4.1 Statistical Outlier Detection Using Z-Score \\\n",
        "2.4.2 Using Interquartile Range and Boxplots \\\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "O9hpxolNxS2F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.1. Data Visualization\n"
      ],
      "metadata": {
        "id": "CeUpqnQXRAvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data visualization is the process of transforming data into visual representations, such as charts, graphs, and maps. It is a powerful tool that can be used to communicate information clearly and concisely.\n",
        "\n",
        "In artificial intelligence (AI), data visualization is used to:\n",
        "\n",
        "- ***Explore and understand data***: Data visualization can be used to explore data and identify patterns and trends. This can be helpful for tasks such as data mining and machine learning. \\\n",
        "- ***Communicate insights***: Data visualization can be used to communicate insights from data to stakeholders. This can be helpful for tasks such as reporting and decision-making. \\\n",
        "- ***Generate hypotheses***: Data visualization can be used to generate hypotheses about the data. This can be helpful for tasks such as research and problem-solving. As an example: A researcher can use a scatter plot to visualize the relationship between the number of hours studied and exam scores. If there is a positive correlation, the researcher can hypothesize that more hours studied leads to higher exam scores. \\\n",
        "- ***Validate models***: Data visualization can be used to validate models created by AI algorithms. This can help to ensure that the models are accurate and reliable. For example, A retailer has developed a model to predict customer purchase behavior. They can use a heatmap to visualize the relationship between customer features and purchase behavior. The darker the color, the stronger the relationship."
      ],
      "metadata": {
        "id": "X84tjML6G7MQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.1.1 Using Matplotlib\n",
        "Matplotlib package is a data visualization library that is used to create professional figures and plots. To make a plot, you need first to import the **pyplot** sub-module and then use the **plot** method with proper arguments."
      ],
      "metadata": {
        "id": "qgx9TYqdoW63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# generating a sinwwave signal\n",
        "t = np.arange(0, 1, 0.001)\n",
        "sig = np.sin(2 * np.pi * 2 * t)\n",
        "\n",
        "plt.plot(t,sig)\n",
        "plt.xlabel('Time (sec)')\n",
        "plt.ylabel('Amplitude')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RfkuUIfERAP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **Matplotlib** package can also be used to plot multiple axes in the same figure or two curves on the same axis."
      ],
      "metadata": {
        "id": "2v2MnhptUbeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generating two sinwwave signals\n",
        "t = np.arange(0, 2, 0.001)\n",
        "sig1 = np.sin(2 * np.pi * 2 * t)\n",
        "sig2 = np.sin(2 * np.pi * 2 * t - np.pi/6)\n",
        "sig3 = np.sin(2 * np.pi * 2 * t + np.pi/4)\n",
        "\n",
        "fig, axs = plt.subplots(2, 1)\n",
        "axs[0].plot(t, sig1)\n",
        "axs[0].set_xlim(0, 2)\n",
        "axs[0].set_xlabel('Time (sec)')\n",
        "axs[0].set_ylabel('Amplitude')\n",
        "axs[0].grid(True)\n",
        "\n",
        "axs[1].plot(t, sig2, t, sig3)\n",
        "axs[1].set_xlim(0, 2)\n",
        "axs[1].set_xlabel('Time (sec)')\n",
        "axs[1].set_ylabel('Amplitude')\n",
        "axs[1].grid(True)\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zWl7mh7MUiEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.1**: Refer to the matplot documentation at https://matplotlib.org/stable/gallery/color/named_colors.html to plot two curves in the same figure, add markers of specific size, add a legend, and add a figure title."
      ],
      "metadata": {
        "id": "0P1KcwMtSzSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#write you code here\n"
      ],
      "metadata": {
        "id": "M8Z7g3cpYzSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.2**: Create two axes next to each other, and plot a sinwave in one axis and a cosine wave on the other. Add markers, legend, and a title for the figure."
      ],
      "metadata": {
        "id": "qhppNcuyW98e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#write you code here\n"
      ],
      "metadata": {
        "id": "GfTp52lqY2tW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##2.1.2 Using Seaborn\n",
        "Seaborn is another data visualization library based on matplotlib. It provides a high-level interface for drawing informative graphics. To make a plot, you need first to import the **sns** sub-module and then use a specific method with proper arguments. For example you may use the **relplot** method to create relational plots (plots in which the relationship between two or more variables is visually represented)."
      ],
      "metadata": {
        "id": "epEqA2TGX-xN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# generating a sinwwave signal\n",
        "t = np.arange(0, 1, 0.001)\n",
        "sin = np.sin(2 * np.pi * 2 * t)\n",
        "cos = np.cos(2 * np.pi * 2 * t)\n",
        "\n",
        "#Creating a dataframe from the time, sin, and cos curves\n",
        "df = pd.DataFrame({'time':t, 'sin':sin, 'cos':cos})\n",
        "\n",
        "# Create a visualization\n",
        "sns.relplot(data=df,kind=\"line\",x=\"time\",y=\"sin\").set(title='Sinwave')"
      ],
      "metadata": {
        "id": "pzHgCPxj4Nmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also use the **relplot** function to create more advanced visualizations of data. For instance, let's take the \"tips\" dataset as an example. This dataset contains information about tips received by a waiter over a few months in a restaurant. It has details like how much tip was given, the bill amount, whether the person paying the bill is male or female, if there were smokers in the group, the day of the week, the time of day, and the size of the group. To import the dataset use the following code"
      ],
      "metadata": {
        "id": "JEC0igG1FN6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load an example dataset\n",
        "tips = sns.load_dataset(\"tips\")\n",
        "tips.info()"
      ],
      "metadata": {
        "id": "hrkK8xWf7znI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute the following code to view the first 10 rows in the dataset"
      ],
      "metadata": {
        "id": "w1wQvyLBHF8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tips.head()"
      ],
      "metadata": {
        "id": "h9H2zus1HD7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the **relplot()** method helps us understand patterns in the dataset and how different factors might be connected."
      ],
      "metadata": {
        "id": "9WUT7tGmGlhh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a visualization\n",
        "sns.relplot(data=tips,\n",
        "    x=\"total_bill\", y=\"tip\", col=\"time\",\n",
        "    hue=\"smoker\", style=\"smoker\", size=\"size\",\n",
        ")"
      ],
      "metadata": {
        "id": "orIQqAkj78VY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From observing the visualization of the tips dataset, we can infer that as the total bill size grows, the tip value tends to increase proportionally. Additionally, it's apparent that both the total bill and tip value are higher when the group size is larger. **Can you observe any other patterns?**"
      ],
      "metadata": {
        "id": "DLhe12ulH1XP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.3**: Load a dataset from the sns repository and then use the **relplot()** method to visualize and understand patterns in the dataset. You may list the datasets in the sns repository using the **sns.get_dataset_names()** method."
      ],
      "metadata": {
        "id": "iqSms4zuMgJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#write you code here\n"
      ],
      "metadata": {
        "id": "fpEmSnZfN1Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **histplot()** is another method in the **sns** submodule that can be used to plot univariate or bivariate histograms to show distributions of datasets.\n",
        "\n",
        "**Note**: A histogram is a graphical representation of data that shows the distribution of values within a dataset. It is a way to visualize how often different values or ranges of values occur in the data. In a histogram, the x-axis represents the possible values or ranges of values, and the y-axis represents the frequency or count of those values in the dataset. Each bar in the histogram represents a group or \"bin\" of values, and the height of the bar corresponds to the number of data points that fall into that group."
      ],
      "metadata": {
        "id": "mcJwy388KeE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(figsize=(16, 4),ncols=3)\n",
        "\n",
        "#Create histograms displaying the distribution of tip values\n",
        "sns.histplot(data=tips, x=\"tip\", ax=axs[0])\n",
        "\n",
        "#Create histograms displaying the distribution of tip values based on the time of day\n",
        "sns.histplot(data=tips, x=\"tip\", hue=\"time\",ax=axs[1])\n",
        "\n",
        "#Create histograms displaying the distribution of tip values based on the time of day, and incorporate the actual distribution curve.\n",
        "sns.histplot(data=tips, x=\"tip\", hue=\"time\",ax=axs[2], kde=True)"
      ],
      "metadata": {
        "id": "YGHY8yc1KBHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.4**: Create a histogram plot for the dataset you loaded in task 2.3 and incorporate the actual distribution curve."
      ],
      "metadata": {
        "id": "Ygl_-2kGN4t6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#write you code here\n"
      ],
      "metadata": {
        "id": "Pam0Kol2N4BI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.5**: Use the **scatterplot()** method within the **sns** submodule and the **subplots()** method within the **matplotlib** submodule to generate visual representations for the dataset you loaded in step 2.3."
      ],
      "metadata": {
        "id": "KuWQRpWYRcWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#write you code here\n"
      ],
      "metadata": {
        "id": "D8-UGkshR53b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.1.3 Using Pandas\n",
        "Data visualization using pandas is a common task in data analysis and manipulation. As explained in experiment #1, pandas provides an easy-to-use DataFrame structure that allows you to store, manipulate, and analyze data efficiently. When combined with data visualization libraries like Matplotlib or Seaborn, pandas can generate a wide range of visualizations to explore and communicate insights from your data. In this section, we will present few types of visualizations that can be created using pandas.\n",
        "\n",
        "Let us create a sample DataFrame using the folowing code."
      ],
      "metadata": {
        "id": "xbMyQsI83JZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Create a sample DataFrame\n",
        "data = {\n",
        "    'Category': ['Fruits','Fruits','Fruits','Fruits','Fruits', 'Vegetables','Vegetables','Vegetables','Vegetables','Vegetables','Grains','Grains','Grains'],\n",
        "    'Item': ['Apple', 'Banana', 'Orange', 'Mango', 'Grapes', 'Spinach', 'Tomato', 'Cucumber','Cauliflower','Eggplant','Rice', 'Wheat', 'Corn'],\n",
        "    'Weight': [200, 150, 250, 150, 200, 120, 200, 120, 120, 250, 120, 300, 300],\n",
        "    'Cost': [0.5, 0.3, 0.2, 2.5, 1.0, 1.5, 0.3, 0.3, 0.5, 1.2, 1.6, 0.8, 0.5],\n",
        "    'Calories': [95, 23, 205, 335, 120, 33, 50, 250, 350, 300, 420, 200, 250]\n",
        "}\n",
        "\n",
        "\n",
        "food = pd.DataFrame(data)\n",
        "food.head()"
      ],
      "metadata": {
        "id": "7pAXMkqO3cTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Line Plot**: To create a line plot, you can use the **plot()** method on the DataFrame."
      ],
      "metadata": {
        "id": "-NNOH_5s3jzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "food.plot(x='Weight', y='Cost', kind='line', color='red')\n",
        "plt.xlabel('Weight')\n",
        "plt.ylabel('Cost')\n",
        "plt.title('Line Plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BQnXrEic33uH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scatter Plot**: To create a scatter plot use the **plot()** method with **kind='scatter'**."
      ],
      "metadata": {
        "id": "-rQaamEr47Jp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "food.plot(x='Weight', y='Cost', kind='scatter')\n",
        "plt.xlabel('Weight')\n",
        "plt.ylabel('Cost')\n",
        "plt.title('Scatter Plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wywDQWLJ4-oG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Histogram**: To create a histogram use the **plot()** method with **kind='hist'**."
      ],
      "metadata": {
        "id": "0Mcuh2_V5GjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "food['Cost'].plot(kind='hist', bins=10)\n",
        "plt.xlabel('Cost')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aZTZRaSj5HOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bar Plot**: To create a bar plot use the **plot()** method with **kind='bar'**."
      ],
      "metadata": {
        "id": "ywNe6QYC5dpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "food.plot(x='Weight', y='Cost', kind='bar')\n",
        "plt.xlabel('Weight')\n",
        "plt.ylabel('Cost')\n",
        "plt.title('Bar Plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XktH-ybF5eXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also utilize the **groupby()** function along with the **plot()** function with the **kind='bar'** option to aggregate column values and generate insightful bar visualizations."
      ],
      "metadata": {
        "id": "WCSJqi9t6d2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grouping by 'Category' and summing 'Cost'\n",
        "grouped_data = food.groupby('Category')['Cost'].sum()\n",
        "\n",
        "# Creating a bar plot\n",
        "grouped_data.plot(kind='bar')\n",
        "plt.xlabel('Product Category')\n",
        "plt.ylabel('Total Sales Amount')\n",
        "plt.title('Total Sales Amount by Product Category')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AQMzb6rB6vJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.7**: Create a histogram for the food weights in the Food DataFrame defined in this section."
      ],
      "metadata": {
        "id": "-fDg2Vb0AT-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#write you code here\n"
      ],
      "metadata": {
        "id": "kyyePadvBEeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.8**: Generate informative bar charts illustrating the calorie distribution across food categories using the Food DataFrame introduced in this section."
      ],
      "metadata": {
        "id": "h71DGLEoBGeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#write you code here\n"
      ],
      "metadata": {
        "id": "dtdrJ8E_BIbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.1.4 Boxplot"
      ],
      "metadata": {
        "id": "wu8ys7wDlXCQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A boxplot is a graphical representation that provides insights into the distribution and variability of data. It help us visualize the spread and central tendency of the data. In a boxplot, a rectangular \"box\" is drawn to represent the interquartile range (IQR), which spans from the first quartile (Q1) to the third quartile (Q3) of the data. The first quartile (Q1) represents the point where a quarter (25%) of the data values fall below when arranged in increasing order. On the other hand, the third quartile (Q3), marks the threshold beneath which three-quarters (75%) of the data values are situated when organized in increasing order."
      ],
      "metadata": {
        "id": "26YP8_o-lKbk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The boxplot can be created using the **boxplot()** method within the **panda** package (panada DataFrame)."
      ],
      "metadata": {
        "id": "XPy2umIamdBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tips.boxplot(by ='day', column =['total_bill'], grid = False)"
      ],
      "metadata": {
        "id": "j5W6lvqFmn1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The boxplot can also be generated using the **boxplot()** method within the **sns** submodule."
      ],
      "metadata": {
        "id": "ZSinC8BkmonZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x = 'day', y = 'total_bill', data = tips)"
      ],
      "metadata": {
        "id": "cBYQCH76mpCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.9**: Use the **boxplot()** method within the **sns** submodule to generate boxplot for the dataset you loaded in step 2.3."
      ],
      "metadata": {
        "id": "rc-hO9ownzIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#write you code here\n"
      ],
      "metadata": {
        "id": "v3eauRDIn0iJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.2 Descriptive statistics"
      ],
      "metadata": {
        "id": "LwOUX2z0uxrc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descriptive statistics involves analyzing and summarizing data to gain insights into its central tendencies, variability, and overall distribution. It playes a crucial role in machine learning for many reasons including data understanding and exploration and data cleaning and preprocessing. In data understanding and exploration, descriptive statistics provide an initial overview of the dataset, helping you understand its distribution, central tendencies, and variability. This exploration phase is vital for identifying data patterns, anomalies, and potential issues that might impact the quality of your machine learning models. While in data cleaning and preprocessing, descriptive statistics help you identify missing values, outliers, and inconsistencies that need to be addressed. Cleaning and preprocessing ensure that your machine learning model receives accurate and reliable input data.\n",
        "\n",
        "In this section, we will demonstrate descriptive statistics by working with the data stored in the **ENCS5141_Exp2_DescriptiveStatistics.csv** and **ENCS5141_Exp2_ShapeDistribution.csv** files. These files can be found in the GitHub repository located at https://github.com/mkjubran/ENCS5141Datasets. To clone the repository, you can execute the following code:"
      ],
      "metadata": {
        "id": "tTANyDulGjU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ./ENCS5141Datasets\n",
        "!git clone https://github.com/mkjubran/ENCS5141Datasets.git"
      ],
      "metadata": {
        "id": "L5OrPelJ59pV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we will read the ENCS5141_Exp2_DescriptiveStatistics.csv into a DataFrame use the **pd.read_csv()** method."
      ],
      "metadata": {
        "id": "WP3O2MYP6Kbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/ENCS5141Datasets/ENCS5141_Exp2_DescriptiveStatistics.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "jhaaTOwx6PZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.2.1 Central Tendency\n",
        "The following measures are employed to assess the central tendency of a distribution of data: \\\n",
        "**Mean**: The average value of the data. \\\n",
        "**Median**: The middle value when the data is sorted. \\\n",
        "**Mode**: The value that appears most frequently in the data. \\\n",
        "Pandas provides methods like **mean()**, **median()**, and **mode()** to calculate these measures."
      ],
      "metadata": {
        "id": "lgk-YRqSwJ7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the mean for each column in the DataFrame\n",
        "Mean = df.mean()\n",
        "print(f\"Mean values: {Mean.values}\\n\")\n",
        "\n",
        "# Calculate the median for each column in the DataFrame\n",
        "Median = df.median()\n",
        "print(f\"Median values: {Median.values}\\n\")\n",
        "\n",
        "# Calculate the mode for each column in the DataFrame\n",
        "Mode = df.mode().values\n",
        "print(f\"Mode values: {Mode}\\n\")"
      ],
      "metadata": {
        "id": "mKrLrxrpAtVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To gain a visual understanding of the central tendency measures of the loaded data, we can utilize the **sns.histplot()** method to display the column distributions of the dataset."
      ],
      "metadata": {
        "id": "8NtcPTRfevMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "fig, axs = plt.subplots(figsize=(18, 5),ncols=3,nrows=2)\n",
        "\n",
        "#Create histograms displaying the distribution of specific column values and incorporate the actual distribution curve.\n",
        "sns.histplot(data=df, x=\"col_1\", ax=axs[0,0], kde=True);axs[0,0].set(xlim=(-10, 10))\n",
        "axs[0,0].set_title(f\"Mean = {Mean[0]}\\n Median = {Median[0]} \\n Mode = {Mode[0,0]}\")\n",
        "\n",
        "sns.histplot(data=df, x=\"col_2\", ax=axs[0,1], kde=True);axs[0,1].set(xlim=(-10, 10))\n",
        "axs[0,1].set_title(f\"Mean = {Mean[1]}\\n Median = {Median[1]} \\n Mode = {Mode[0,1]}\")\n",
        "\n",
        "sns.histplot(data=df, x=\"col_3\", ax=axs[0,2], kde=True);axs[0,2].set(xlim=(-10, 10))\n",
        "axs[0,2].set_title(f\"Mean = {Mean[2]}\\n Median = {Median[2]} \\n Mode = {Mode[0,2]}\")\n",
        "\n",
        "sns.histplot(data=df, x=\"col_4\", ax=axs[1,0], kde=True);axs[1,0].set(xlim=(-10, 10))\n",
        "axs[1,0].set_title(f\"Mean = {Mean[3]}\\n Median = {Median[3]} \\n Mode = {Mode[0,3]}\")\n",
        "\n",
        "sns.histplot(data=df, x=\"col_5\", ax=axs[1,1], kde=True);axs[1,1].set(xlim=(-10, 10))\n",
        "axs[1,1].set_title(f\"Mean = {Mean[4]}\\n Median = {Median[4]} \\n Mode = {Mode[0,4]}\")\n",
        "\n",
        "sns.histplot(data=df, x=\"col_6\", ax=axs[1,2], kde=True);axs[1,2].set(xlim=(-10, 10))\n",
        "axs[1,2].set_title(f\"Mean = {Mean[5]}\\n Median = {Median[5]} \\n Mode = {Mode[0,5]}\")\n",
        "\n",
        "fig.subplots_adjust(hspace=1)"
      ],
      "metadata": {
        "id": "xZcXW_hWerTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: The bell-like shapes above (especialy for col_1, col_2, col_3, and 'col_4') are for normal distibutions also known as a Gaussian distribution. In a normal distribution, the mean, median, and mode are all centered at the same value. This central value is the highest point on the symmetrical curve. It's important to note that the mean, median, and mode being the same in a normal distribution is a characteristic that distinguishes it from other types of distributions."
      ],
      "metadata": {
        "id": "2q7e_4E0mReR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.10**: Explain how the mean, median, and mode measure central tendency by observing the distribution curves depicted in the graphs above."
      ],
      "metadata": {
        "id": "WV1OAeJaCT7x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.2.2 Variation\n",
        "The subsequent metrics are utilized to evaluate the dispersion of data distribution:\\\n",
        "**Variance**: A measure of how much the data points deviate from the mean.\\\n",
        "**Standard Deviation**: The square root of the variance, indicating the spread of data. \\\n",
        "Pandas provides functions such as **var()** and **std()** for computing the variance and standard deviation of columns within the DataFrame."
      ],
      "metadata": {
        "id": "pgfgXhCowOJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the variance for each column in the DataFrame\n",
        "Variance = df.var()\n",
        "print(f\"Variance values: {Variance.values}\\n\")\n",
        "\n",
        "# Calculate the standard deviaton for each column in the DataFrame\n",
        "STD = df.std()\n",
        "print(f\"Standard deviation values: {STD.values}\\n\")"
      ],
      "metadata": {
        "id": "7-630KqWhusJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, let us use the use the **sns.histplot()** method to gain a visual understanding of the variation measures of the loaded data."
      ],
      "metadata": {
        "id": "1fk-KPfZje8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "fig, axs = plt.subplots(figsize=(18, 7),ncols=3,nrows=2)\n",
        "\n",
        "#Create histograms displaying the distribution of specific column values and incorporate the actual distribution curve.\n",
        "sns.histplot(data=df, x=\"col_1\", ax=axs[0,0], kde=True);axs[0,0].set(xlim=(-10, 10))\n",
        "axs[0,0].set_title(f\"Mean = {Mean[0]}\\n Median = {Median[0]} \\n Mode = {Mode[0,0]} \\n Variance = {Variance[0]} \\n STD = {STD[0]}\")\n",
        "\n",
        "sns.histplot(data=df, x=\"col_2\", ax=axs[0,1], kde=True);axs[0,1].set(xlim=(-10, 10))\n",
        "axs[0,1].set_title(f\"Mean = {Mean[1]}\\n Median = {Median[1]} \\n Mode = {Mode[0,1]} \\n Variance = {Variance[1]} \\n STD = {STD[1]}\")\n",
        "\n",
        "sns.histplot(data=df, x=\"col_3\", ax=axs[0,2], kde=True);axs[0,2].set(xlim=(-10, 10))\n",
        "axs[0,2].set_title(f\"Mean = {Mean[2]}\\n Median = {Median[2]} \\n Mode = {Mode[0,2]} \\n Variance = {Variance[2]} \\n STD = {STD[2]}\")\n",
        "\n",
        "sns.histplot(data=df, x=\"col_4\", ax=axs[1,0], kde=True);axs[1,0].set(xlim=(-10, 10))\n",
        "axs[1,0].set_title(f\"Mean = {Mean[3]}\\n Median = {Median[3]} \\n Mode = {Mode[0,3]} \\n Variance = {Variance[3]} \\n STD = {STD[3]}\")\n",
        "\n",
        "sns.histplot(data=df, x=\"col_5\", ax=axs[1,1], kde=True);axs[1,1].set(xlim=(-10, 10))\n",
        "axs[1,1].set_title(f\"Mean = {Mean[4]}\\n Median = {Median[4]} \\n Mode = {Mode[0,4]} \\n Variance = {Variance[4]} \\n STD = {STD[4]}\")\n",
        "\n",
        "sns.histplot(data=df, x=\"col_6\", ax=axs[1,2], kde=True);axs[1,2].set(xlim=(-10, 10))\n",
        "axs[1,2].set_title(f\"Mean = {Mean[5]}\\n Median = {Median[5]} \\n Mode = {Mode[0,5]} \\n Variance = {Variance[5]} \\n STD = {STD[5]}\")\n",
        "\n",
        "fig.subplots_adjust(hspace=1.0)"
      ],
      "metadata": {
        "id": "6P0lNAFhjfVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.11**: Explain how the variance and standard deviation measure the variation by observing the distribution curves depicted in the graphs above."
      ],
      "metadata": {
        "id": "hl4xGzUnlEuT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.2.3 Shape of Distribution\n",
        "Skewness and kurtosis are statistical measures that provide insights into the shape of a distribution: \\\n",
        "**Skewness**: Measures the asymmetry of the data distribution. \\\n",
        "**Kurtosis**: Measures the peakedness of the data distribution. \\\n",
        "If you need to calculate skewness and kurtosis for multiple columns or across the entire DataFrame, you can use **df.skew()** and **df.kurtosis()** without specifying a column name. These functions return Series with the skewness or kurtosis values for each column. To grasp the connection between skewness and kurtosis and the form of the distribution, we'll employ the dataset contained within ENCS5141_Exp2_ShapeDistribution.csv."
      ],
      "metadata": {
        "id": "JRbRqzb8wPMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "df_shape = pd.read_csv(\"/content/ENCS5141Datasets/ENCS5141_Exp2_ShapeDistribution.csv\")\n",
        "df_shape.head()\n",
        "\n",
        "# Calculate the Skewness for each column in the DataFrame\n",
        "Skewness = df_shape.skew()\n",
        "print(f\"Skewness values: {Skewness.values}\\n\")\n",
        "\n",
        "# Calculate the Kurtosis for each column in the DataFrame\n",
        "Kurtosis = df_shape.kurtosis()\n",
        "print(f\"Kurtosis values: {Kurtosis.values}\\n\")\n",
        "\n",
        "fig, axs = plt.subplots(figsize=(18, 7),ncols=3)\n",
        "\n",
        "#Create histograms displaying the distribution of specific column values and incorporate the actual distribution curve.\n",
        "sns.histplot(data=df_shape, x=\"col_1\", ax=axs[0], kde=True);axs[0].set(xlim=(-10, 10))\n",
        "axs[0].set_title(f\"Skewness = {Skewness[0]} \\n Kurtosis = {Kurtosis[0]}\")\n",
        "\n",
        "sns.histplot(data=df_shape, x=\"col_2\", ax=axs[1], kde=True);axs[1].set(xlim=(-10, 10))\n",
        "axs[1].set_title(f\"Skewness = {Skewness[1]} \\n Kurtosis = {Kurtosis[1]}\")\n",
        "\n",
        "sns.histplot(data=df, x=\"col_3\", ax=axs[2], kde=True);axs[2].set(xlim=(-10, 10))\n",
        "axs[2].set_title(f\"Skewness = {Skewness[2]} \\n Kurtosis = {Kurtosis[2]}\")"
      ],
      "metadata": {
        "id": "Hyo5b3o-oa8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**:\n",
        "- If skewness is close to 0, the distribution is approximately symmetric.\n",
        "- If skewness is negative, the tail is longer on the left side (left-skewed).\n",
        "- If skewness is positive, the tail is longer on the right side (right-skewed).\n",
        "- If kurtosis is close to 3, the distribution has similar tails as a normal distribution.\n",
        "- If kurtosis is less than 3, the distribution has lighter tails and a flatter peak (a lighter-tailed distribution has a lower probability of producing values that are far from the mean compared to a distribution with heavier tails. This means that extreme values or outliers are less common in a dataset that follows a lighter-tailed distribution).\n",
        "- If kurtosis is greater than 3, the distribution has heavier tails and a sharp peak (a heavy-tailed distribution is one where the tail of the distribution decays more slowly than that of a normal distribution, implying that extreme values are more likely to occur.)."
      ],
      "metadata": {
        "id": "VktQQlKbqhty"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.11**: Explain how the skewness and kurtosis are related to the shape of the distribution by observing the distribution curves depicted in the graphs above,"
      ],
      "metadata": {
        "id": "A1nPaRqRC1U0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.12**: Compute the skewness and kurtosis of the dataset in ENCS5141_Exp2_DescriptiveStatistics.csv, and create relevant visualizations to showcase the connection between the skewness, kurtosis, and the underlying distribution shape of the data."
      ],
      "metadata": {
        "id": "Hn4tbF0pDDCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#write you code here\n"
      ],
      "metadata": {
        "id": "68dNcwWSDD5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.2.4 Quantiles\n",
        "**Percentiles**: Values below which a given percentage of data falls. \\\n",
        "**Interquartile Range (IQR)**: The range between the 25th and 75th percentiles. \\\n",
        "Pandas offers the **quantile()** method for calculating percentiles. For instance, to obtain the 75th percentile, employ quantile(0.75), and for the 25th percentile, utilize quantile(0.25). Similarly, to compute the interquartile range (IQR), you can apply the formula quantile(0.75) - quantile(0.25). This is demonstrated in the code snippet below."
      ],
      "metadata": {
        "id": "6kSUL6Ewybb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute percentiles using Pandas quantile() function\n",
        "percentile_25 = df_shape['col_1'].quantile(0.25)\n",
        "percentile_75 = df_shape['col_1'].quantile(0.75)\n",
        "\n",
        "print(\"25th Percentile:\", percentile_25)\n",
        "print(\"75th Percentile:\", percentile_75)\n",
        "\n",
        "# Compute interquartile range (IQR)\n",
        "iqr = percentile_75 - percentile_25\n",
        "print(\"Interquartile Range (IQR):\", iqr)"
      ],
      "metadata": {
        "id": "QfODemGlCZdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.3. Handling Missing Data\n"
      ],
      "metadata": {
        "id": "b3uSipo2oOLs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is missing data?** \\\n",
        "Missing data is data that is not available for one or more observations in a dataset. It can occur for a variety of reasons, such as human error, equipment malfunction, or deliberate omission.\n",
        "\n",
        "**Why is handling missing data important?** \\\n",
        "Missing data can have a significant impact on the quality and accuracy of a dataset. If not handled properly, it can lead to biased results and inaccurate conclusions.\n",
        "\n",
        "**How to handle missing data?** \\\n",
        "There are a number of methods for handling missing data. The best method to use will depend on the specific circumstances and the goals of the analysis. Some common methods include: \\\n",
        "\n",
        "- ***Dropping/Deletion***: This method is used if the number of missing values in a row is relatively small, and you believe these rows do not carry essential information for your analysis. Use the **dropna()** method in pandas to remove rows with missing values from your dataset. \\\n",
        "\n",
        "- ***Create a Separate Category***: If missing values represent a distinct category or carry specific meaning in your analysis, assign a special label or category to missing values. This makes them a distinct class in your analysis. \\\n",
        "\n",
        "- ***Imputation***: The imputation of missing values is used if the proportion of missing values is significant, and you believe these data points are valuable for your analysis. You can impute missing values using various techniques:\n",
        " - Mean, Median, or Mode Imputation: Replace missing values with the mean, median, or mode of the observed values in the respective column.\n",
        " - Regression Imputation: Train a regression model to predict missing values based on other features in the dataset.\n",
        " - K-Nearest Neighbors (KNN) Imputation: Impute missing values by averaging the values of the K-nearest neighbors in feature space.\n",
        " - Advanced Imputation Techniques: Utilize more advanced methods like decision trees, random forests, or matrix factorization.\n",
        "\n",
        "- ***Modeling***: When you have sufficient data with complete values and believe that missing values can be predicted accurately. Treat predicting missing values as a separate machine learning task. Train a model to predict missing values based on other features in your dataset. This can be a more complex method, but it can also be more accurate than imputation.\n",
        "\n",
        "- ***Time-Series Interpolation***: This method can be applied when dealing with time-series data with missing values. Interpolate missing values based on the observed values in the time series. Methods like linear interpolation or cubic spline interpolation can be used.\n",
        "\n",
        "- ***Use Domain Knowledge***: Employ your domain expertise to make informed choices regarding missing data. Utilize your specialized knowledge to decide the most suitable approach for managing missing values, taking into account the dataset's context.\n",
        "\n",
        "**What is the impact of missing data**? \\\n",
        "The impact of missing data on a dataset will depend on the following factors: \\\n",
        "- ***The amount of missing data***: The more missing data there is, the greater the impact it will have. \\\n",
        "- ***The type of missing data***: Missing data can be either missing completely at random (MCAR), missing at random (MAR), or missing not at random (MNAR). MCAR is the least harmful type of missing data, while MNAR is the most harmful. \\\n",
        "- ***The goals of the analysis***: The goals of the analysis will also affect the impact of missing data. If the analysis is sensitive to missing data, then it is important to use a method that will minimize its impact.\n",
        "\n",
        "**Case on Missing Data: Clinical Trial on Drug Efficacy**\n",
        "\n",
        "Imagine a pharmaceutical company conducts a clinical trial to evaluate the efficacy of a new drug in treating a particular medical condition. The trial spans several months and involves regular check-ups and tests for enrolled patients.\n",
        "\n",
        "Now, let's consider the potential for missing data:\n",
        "\n",
        "- ***Patient Dropout***: During the trial, some patients may drop out for various reasons, such as experiencing side effects, personal decisions, or being lost to follow-up. As a result, their data is missing from later time points.\n",
        "\n",
        "- ***Incomplete Records***: Not all patients may complete every scheduled test or check-up. Some may miss appointments, leading to missing test results or medical data.\n",
        "\n",
        "- ***Measurement Errors***: Occasionally, technical issues or measurement errors can result in missing or unreliable data points for certain patients.\n",
        "\n",
        "In this case, the analysis of the drug's efficacy could be sensitive to the missing data. If the missing data isn't handled appropriately, it may bias the results or lead to incorrect conclusions about the drug's effectiveness. For example:\n",
        "\n",
        "- If patients who experienced severe side effects and dropped out are not properly accounted for, the analysis might underestimate the drug's potential risks.\n",
        "\n",
        "- Missing test results could affect the assessment of treatment outcomes and the drug's impact on the condition being studied."
      ],
      "metadata": {
        "id": "ivbGjYqn_xhw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.2.1 Handling Missing Data in Cardiovascular Disease Dataset"
      ],
      "metadata": {
        "id": "KvF4i01yzPZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we will demonstrate how to clean data. We will use a modified version of the cardiovascular dataset from Kaggle (https://www.kaggle.com/code/sulianova/eda-cardiovascular-data/data). Cardiovascular disease is a group of diseases that affect the heart and blood vessels. It is the leading cause of death in the world. This dataset contains 70,000 records of patient data with 12 features. The target variable \"cardio\" is equal to 1 if the patient has cardiovascular disease and 0 if the patient is healthy. The target variable is the variable that we are trying to predict. In this case, the target variable is \"cardio\", which indicates whether the patient has cardiovascular disease."
      ],
      "metadata": {
        "id": "AQtZ-SFvErkA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initially, we need to import the numpy, pandas, and matplotlib libraries that will be used during data cleaning."
      ],
      "metadata": {
        "id": "p8IHjsB_NJ5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import rcParams\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "LXO2d36eNJSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Accessing the Dataset***\n",
        "\n",
        "The cardiovascular disease dataset is saved in a file named **cardio_train_modified.csv**. This file is located within the GitHub repository that was accessed in section 2.2. To read and display the dataset, you can execute the following code."
      ],
      "metadata": {
        "id": "Y_clq04sKGn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read the dataset using pandas\n",
        "df_cardio = pd.read_csv(\"/content/ENCS5141Datasets/ENCS5141_Exp2_Cardio_train_modified.csv\",sep=\";\")\n",
        "\n",
        "# display the first 5 lines\n",
        "df_cardio.head()"
      ],
      "metadata": {
        "id": "B9mKb_m_LtJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Displaying Data Information and Checking NAN***\n",
        "\n",
        "To display the content of the data and type of features use the info() method in pandas. This method provides a concise summary of the DataFrame's structure, including column names, data types, and the count of non-null values. It's valuable for getting a quick overview of the dataset but doesn't directly reveal missing values."
      ],
      "metadata": {
        "id": "Ea4lRShuYjBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cardio.info()"
      ],
      "metadata": {
        "id": "y2UGE3RsYoLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here the DataFrame consists of 70000 rows with 12 variables (features). Ten features are numerical and two features are objects (gender, smoke). We notice that for some of the features the number of non-null values does not equal 70000 which means that some feature values in the data are missing. We can get the exact number of missing values for each feature using the isnull() method. The isnull(), when applied to a DataFrame, returns a DataFrame of the same shape with True in places where values are missing (NaN) and False where values are present. You can then use methods like sum() to count missing values in each column."
      ],
      "metadata": {
        "id": "HhqXkiZra8PN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cardio.isnull().sum()"
      ],
      "metadata": {
        "id": "kXV9LNxccgFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may also obtain the number and percentage of patients' records that has one or more missing values using the **isnull().any()** method in pandas. This method is used to check if there are any missing values (NaN or None) in each column of a DataFrame. It returns a Series that indicates whether any missing values are present in each column."
      ],
      "metadata": {
        "id": "49UJsVbZ48ZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_cardio.isnull().any(axis=1).sum())\n",
        "print(100*df_cardio.isnull().any(axis=1).sum()/df_cardio.shape[0],'%')"
      ],
      "metadata": {
        "id": "BR5M6mwW4873"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the records with missing values."
      ],
      "metadata": {
        "id": "XRh5km2u7hzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cardio[df_cardio.isnull().any(axis=1)]"
      ],
      "metadata": {
        "id": "sYLytc857Yp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handling Missing Data by Dropping**\n",
        "\n",
        "To identify records in which all features and the target value are missing (empty records) use the **isnull().all()** method in pandas. This method is used to check if all values in a DataFrame or Series are missing (NaN or None)."
      ],
      "metadata": {
        "id": "9KTlEionQg28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of empty records = {df_cardio.isnull().all(axis=1).sum()}\")\n",
        "df_cardio[df_cardio.isnull().all(axis=1)]\n"
      ],
      "metadata": {
        "id": "0HWUxnj1QcF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the number of these records is very small compared to the size of the dataset, we will drop them. To drop these empty records use the **dropna(how='all')** in pandas. This method is used to remove rows from a DataFrame where all values in those rows are missing (NaN or None). It essentially drops rows that are completely empty. The **inplace=True** parameter means that the operation will modify the original DataFrame directly."
      ],
      "metadata": {
        "id": "slsyL5n6B5e-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cardio.dropna(how='all', inplace=True)\n",
        "print(df_cardio.isnull().sum())"
      ],
      "metadata": {
        "id": "IRj5nkzjC31b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we contrast the count of NaN features before and after the final step, it becomes apparent that the three empty records have been eliminated. Additionally, we observe that the count of missing values for the features 'weight,' 'ap_hi,' 'ap_lo,' and 'gluc' is quite small compared to the number of records. Therefore, the best decision is to remove these records from the dataset."
      ],
      "metadata": {
        "id": "DI_uEUsMDWRa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.13**: Write a code for the removal of records containing missing values in the 'weight,' 'ap_hi,' 'ap_lo,' and 'gluc' features. Afterwards, validate that the DataFrame no longer contains any missing values in these specified features. You may use the 'dropna()' method with the 'subset' parameter for this purpose. *The dropna() method in pandas can be used with the subset parameter to drop rows that contain missing values in specific columns of a DataFrame.*"
      ],
      "metadata": {
        "id": "abVkvJrmYOYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#write you code here\n"
      ],
      "metadata": {
        "id": "7GBR028yX6zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code is required to advance the experiment, irrespective of task 2.13.\n",
        "df_cardio.dropna(subset=['weight'], inplace=True)\n",
        "df_cardio.dropna(subset=['ap_hi'], inplace=True)\n",
        "df_cardio.dropna(subset=['ap_lo'], inplace=True)\n",
        "df_cardio.dropna(subset=['gluc'], inplace=True)\n",
        "print(df_cardio.isnull().sum())"
      ],
      "metadata": {
        "id": "5qMfyt-sWtQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handling Missing Data by Imputation**"
      ],
      "metadata": {
        "id": "YEDfpmFJcUOV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 'gender' feature consists of 'male' and 'female' strings, but we have numerous missing values. One approach is to remove all records where the 'gender' feature is missing. However, this would entail discarding approximately 1.4% of the records, a decision that should be made by domain experts. In this section, we will opt for imputation as our method for addressing missing values in the 'gender' feature."
      ],
      "metadata": {
        "id": "tF97qFWPAiJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The number of records where gender is missing equals {df_cardio.isnull()['gender'].sum()}\")\n",
        "print(f\"The proportion of records where gender is missing equals {100*df_cardio.isnull()['gender'].sum()/df_cardio.shape[0]}%\")"
      ],
      "metadata": {
        "id": "8cRP3C98MZ5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One approach is to employ mode imputation, which is suitable for addressing missing categorical or nominal data. In this method, missing values are substituted with the mode, representing the most frequently occurring category or class within the variable. To provide a visual representation of the dataset's gender distribution, we will create a bar plot showing the proportions of 'male' and 'female'."
      ],
      "metadata": {
        "id": "gojREl-wiK39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grouping by 'gender' and summing 'Cost'\n",
        "df_cardio_gender = df_cardio.groupby('gender')['id'].count()\n",
        "\n",
        "# Creating a bar plot\n",
        "df_cardio_gender.plot(kind='bar')\n",
        "plt.xlabel('Gender')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "print(df_cardio.groupby('gender')['id'].count() / df_cardio['gender'].shape[0])"
      ],
      "metadata": {
        "id": "F1-UiFwkgV7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An alternative method involves attempting to identify a correlation between the 'gender' feature and the other features within the DataFrame. Subsequently, the missing values in the 'gender' feature can be determined based on the values of other attributes within the same record. Before calculating this correlation, we'll convert the non-numeric features into numeric ones. If you find the conversion process or the accompanying code below unclear, there's no need to worry, as we will explore this in detail in the upcoming experiment. Following the conversion, we will utilize the **corr()** method in pandas to assess the correlation.\""
      ],
      "metadata": {
        "id": "DSMzU-LekIwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# create a copy of the original DataFrame to mainting the original DataFrame\n",
        "df_cardio_corr=df_cardio.copy()\n",
        "\n",
        "\n",
        "# Initialize the LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the categorical feature\n",
        "df_cardio_corr['gender_encoded'] = label_encoder.fit_transform(df_cardio_corr['gender'])\n",
        "df_cardio_corr['smoke_encoded'] = label_encoder.fit_transform(df_cardio_corr['smoke'])\n",
        "\n",
        "# Drop categorical features\n",
        "df_cardio_corr.drop(['gender','smoke'],axis=1,inplace=True)\n",
        "\n",
        "df_cardio_corr.corr()"
      ],
      "metadata": {
        "id": "5r9xaRBOoBsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be observed, the 'gender' (encoded as 'gender_encoded') feature exhibits the highest correlation with the continuous 'height' feature and also has some correlation with the weight and alco features. This suggests that K-Nearest Neighbors (KNN) imputation could be used to estimate missing values for the gender feature. However, since K-Nearest Neighbors (KNN) technique have not been covered in this laboratory, we will not utilize this method and will instead continue with the mode imputation approach described earlier. We will use the **fillna()** method in pandas to replace missing gender values."
      ],
      "metadata": {
        "id": "-ulq0Jhnqzkg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be observed, the 'gender' (encoded as 'gender_encoded') feature exhibits the highest correlation with the continuous 'height' feature and also has some correlation with the weight and alco features. This suggests that K-Nearest Neighbors (KNN) imputation could be used to estimate missing values for the gender feature. However, since K-Nearest Neighbors (KNN) technique have not been covered in this laboratory, we will not utilize this method. Let's attempt to examine how the 'height' feature is distributed for each gender."
      ],
      "metadata": {
        "id": "ugczOFbl9Mq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create histograms displaying the distribution of height values based on the gender, and incorporate the actual distribution curve.\n",
        "sns.histplot(data=df_cardio, x=\"height\", hue=\"gender\", kde=True)\n",
        "plt.xlim([135, 195])\n",
        "\n",
        "print(df_cardio.groupby('gender')['height'].mean())\n",
        "\n",
        "print(df_cardio.groupby('gender')['height'].std())"
      ],
      "metadata": {
        "id": "IuVpX2PK7mci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the distribution, it's noticeable that male records tend to have higher heights, while female records tend to have lower heights. A possible threshold for the 'height' feature between males and females could be estimated as 167. Consequently, any missing gender value with a height greater than or equal to 167 will be assigned as male, while any height less than 167 will be assigned as female."
      ],
      "metadata": {
        "id": "T7d_xdOc-jlo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a copy of the df_cardio DataFrame where the height feature is greater or equal to 167\n",
        "df_cardio_G167 = df_cardio.loc[df_cardio['height']>=167].copy()\n",
        "\n",
        "# Create a copy of the df_cardio DataFrame where the height feature is less that 167\n",
        "df_cardio_L167 = df_cardio.loc[df_cardio['height']<167].copy()\n",
        "\n",
        "# Create a copy of the df_cardio DataFrame where the height feature is missing\n",
        "df_cardio_NaN = df_cardio.loc[df_cardio['height'].isna()].copy()\n",
        "\n",
        "# Fill the gender missing values in the df_cardio_G167 by male\n",
        "df_cardio_G167['gender'].fillna(value='male', inplace=True)\n",
        "\n",
        "# Fill the gender missing values in the df_cardio_L167 by female\n",
        "df_cardio_L167['gender'].fillna(value='female', inplace=True)\n",
        "\n",
        "# Reconstruct the df_cardio DataFrame by concatinaing the three sub Dataframes; df_cardio_L167, df_cardio_G167, and df_cardio_NaN\n",
        "df_cardio = pd.concat([df_cardio_L167, df_cardio_G167, df_cardio_NaN], axis=0)\n",
        "\n",
        "print('The distribution of gender categories in the DataFrame following the replacement of missing values.')\n",
        "print(df_cardio.groupby('gender')['id'].count() / df_cardio['gender'].shape[0])\n",
        "\n",
        "print('\\n\\nInformation about the DataFrame')\n",
        "print(df_cardio.isnull().sum())\n"
      ],
      "metadata": {
        "id": "R8EOz9G4Fxkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gender values remained unfilled because the corresponding height values were also missing. For these missing gender values, we will apply mode imputation such tha the missing values will be replaced with a 'female'."
      ],
      "metadata": {
        "id": "tDCoILHoJTLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Replace gender missing values with 'female'\n",
        "df_cardio['gender'].fillna(value='female', inplace=True)"
      ],
      "metadata": {
        "id": "n6ZfXLW4sj6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To confirm the successful application of mode imputation and ensure that there are no remaining missing values in the 'gender' feature, execute the following code snippet"
      ],
      "metadata": {
        "id": "Uyra2gJQtUiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('The distribution of gender categories in the DataFrame following the replacement of missing values.')\n",
        "print(df_cardio.groupby('gender')['id'].count() / df_cardio['gender'].shape[0])\n",
        "\n",
        "print('\\n\\nInformation about the DataFrame')\n",
        "print(df_cardio.isnull().sum())"
      ],
      "metadata": {
        "id": "ny1fXqvosrek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.14**: Follow the same approach as demonstrated earlier for the 'gender' feature to handle the missing values of the 'smoke' feature"
      ],
      "metadata": {
        "id": "abm9Ndz6uMTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#write you code here\n"
      ],
      "metadata": {
        "id": "UfV1DeE8u9s2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code is required to advance the experiment, irrespective of task 2.14.\n",
        "\n",
        "print('The distribution of smoke categories in the DataFrame.')\n",
        "print(df_cardio.groupby('smoke')['id'].count() / df_cardio['smoke'].shape[0])\n",
        "\n",
        "#Replace smoke missing values with 'female'\n",
        "df_cardio['smoke'].fillna(value='No', inplace=True)\n",
        "\n",
        "print('\\n\\nInformation about the DataFrame')\n",
        "print(df_cardio.isnull().sum())\n"
      ],
      "metadata": {
        "id": "pgBB_uqHvYgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "T10xE3lluMEG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "s20FwNZ9sb4V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Cleaning: 'cholesterol' feature**"
      ],
      "metadata": {
        "id": "V6_acJYAaphM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now handle the missing vlues of the 'cholesterol' feature. This feature takes three values."
      ],
      "metadata": {
        "id": "utfT8QTIas0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.cholesterol.unique()"
      ],
      "metadata": {
        "id": "yWYnuKmca_gM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Is there any correlation with the other features?"
      ],
      "metadata": {
        "id": "nIXSGF2fbL_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.apply(lambda x: x.factorize()[0]).corr()"
      ],
      "metadata": {
        "id": "-f4IeLNObKSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, there is a high correlation between the 'alco' feature and the 'gluc' feature. Let us explore the correlation using crosstab."
      ],
      "metadata": {
        "id": "2UDoMOh7bVIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(pd.crosstab(df['cholesterol'],df['gluc'])/pd.crosstab(df['cholesterol'],df['gluc']).sum())*100"
      ],
      "metadata": {
        "id": "7Tzol3MCb8TW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observe that 81% of the persons with a 'gluc' value of 1.0 has also a 'cholesterol' value of 1.0. We also observe that 65% of the persons with a 'gluc' value of 3.0 has also a 'cholesterol' value of 3.0. And thus we will use these two notes to handle missing values of the 'cholesterol' feature. However, for the persons with a 'gluc' value of 2.0, 43% and 46% have 'cholesterol values of 1.0 and 2.0 which imply that we can not use the 'gluc' value for these persons to handle missing 'cholesterol' values."
      ],
      "metadata": {
        "id": "xfmW1kbscCHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[(df.cholesterol.isnull()) & (df['gluc'] == 1.0),'cholesterol']=1.0\n",
        "df.loc[(df.cholesterol.isnull()) & (df['gluc'] == 3.0),'cholesterol']=3.0"
      ],
      "metadata": {
        "id": "MKrvsUBzeV0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now check the status of missing values"
      ],
      "metadata": {
        "id": "O4Mz0gzKfOwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "i1x4ZN6UfMwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the number of missing values in the 'cholesterol' feature is reduced to 39. Then we will remove these records from the dataset."
      ],
      "metadata": {
        "id": "8pK4Gw0DfYRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.shape)\n",
        "df.dropna(subset=['cholesterol'], inplace=True)\n",
        "print(df.shape)\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "_VZbICsKfo59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Cleaning: 'height' feature**"
      ],
      "metadata": {
        "id": "03DDV6d-Sko4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, for the 'height' feature, is there any correlation with the other features?"
      ],
      "metadata": {
        "id": "Evd_QkGKSwk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.apply(lambda x: x.factorize()[0]).corr()"
      ],
      "metadata": {
        "id": "r0Eu30f9S_KE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, there is a high correlation between the 'height' feature and both the 'gender' and 'weight' features. However, the 'height' feature has a continuous value and we can not deal with it similar to the 'gender' feature'. Instead, we should create a model that predicts the 'height' feature based on the 'gender' and 'weight' features which we will study in the next modules. So, for now, we have two options, either to drop all records where the 'height' feature is NaN or replace all these NaN values with some statistical measure (mean, median) of the 'height' feature. In this notebook, we will replace the NaN values with the median of the values in the 'height' feature."
      ],
      "metadata": {
        "id": "THhi8dlWTdrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.height.median())\n",
        "df['height'].fillna(df.height.median(), inplace=True)\n",
        "print(df.height.median())\n",
        "df.isnull().sum()\n"
      ],
      "metadata": {
        "id": "E5mwkPkPUWxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove Outliers"
      ],
      "metadata": {
        "id": "SxMxobhoAPA5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us have a close look at the statistical properties of the numaric features"
      ],
      "metadata": {
        "id": "xlI-m0y_AVxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "uVxi4Ql9Amfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usually, the 'id' feature will not have outliers, so let us check the 'age' feature. According to the description of the dataset, the age is in days. Let us convert the Age into years so that it is easier to understand and interpret."
      ],
      "metadata": {
        "id": "Dam4xa7eB0LE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['age_years'] = (df['age'] / 365).round().astype('int')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Nb1hkh1SEVkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us have a close look again at the statistical properties of the numaric features"
      ],
      "metadata": {
        "id": "NPNvRQS3EijG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "q8tHIQa_BpSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The minimum age in the datset is about 30 years, the maximum is about 65 years, and the average is about 53.33 years."
      ],
      "metadata": {
        "id": "r90Je8pECGWF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remove Outliers: 'height' and 'weight' feature**\n",
        "\n",
        "Next, let us examine the 'height' feature, the minimum height is 55cm which is too short for the records of persons with a minimum age of 30. Similarly, the maximum height is 250cms which is too rare value for a person's height. So there must be an error in the height feature. Let us also examine the 'weight' feature. The minimum weight is 10 kg which is too low for the records of persons with a minimum age of 30. So again, there must be an error in the 'weight' feature. Let us get the box plot of these two features."
      ],
      "metadata": {
        "id": "lCse5NWlCn2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rcParams['figure.figsize'] = 10, 6\n",
        "df.boxplot(column=['height', 'weight'])"
      ],
      "metadata": {
        "id": "IkRa_lWqD98-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be observed there are outliers, so let us remove weights and heights, that fall below 5% or above 95% of a given range."
      ],
      "metadata": {
        "id": "dF-eZV9gGSKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(df[(df['height'] > df['height'].quantile(0.95)) | (df['height'] < df['height'].quantile(0.05))].index,inplace=True)\n",
        "df.drop(df[(df['weight'] > df['weight'].quantile(0.95)) | (df['weight'] < df['weight'].quantile(0.05))].index,inplace=True)"
      ],
      "metadata": {
        "id": "pGyRMYGUGfpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us get the box plot of these two features again."
      ],
      "metadata": {
        "id": "Y8vVtct3HBQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rcParams['figure.figsize'] = 10, 6\n",
        "df.boxplot(column=['height', 'weight'])"
      ],
      "metadata": {
        "id": "loJ7zofRHAhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be observed, the outliers for the 'height' and 'weight' features are removed."
      ],
      "metadata": {
        "id": "BbgbNWFXA-tS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remove Outliers: 'ap_hi' and 'ap_lo' feature**\n",
        "\n",
        "Similarly, we will do the same for the 'ap_hi' and 'ap_lo' features especially since the blood pressure can not be negative. Below is the box plot for the 'ap_hi' and 'ap_lo' features."
      ],
      "metadata": {
        "id": "BducBqYizknF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rcParams['figure.figsize'] = 10, 6\n",
        "df.boxplot(column=['ap_hi', 'ap_lo'])"
      ],
      "metadata": {
        "id": "1QEKFWT0vy8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we will remove 'ap_hi' and 'ap_hi' features that fall below 5% or above 95% of a given range."
      ],
      "metadata": {
        "id": "ldRDsrTHyy2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(df[(df['ap_hi'] > df['ap_hi'].quantile(0.95)) | (df['ap_hi'] < df['ap_hi'].quantile(0.05))].index,inplace=True)\n",
        "df.drop(df[(df['ap_lo'] > df['ap_lo'].quantile(0.95)) | (df['ap_lo'] < df['ap_lo'].quantile(0.05))].index,inplace=True)"
      ],
      "metadata": {
        "id": "rhK8t4NRyXez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we plot again the box plot of the 'ap_hi' and 'ap_lo' features."
      ],
      "metadata": {
        "id": "luo39o09zAo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rcParams['figure.figsize'] = 10, 6\n",
        "df.boxplot(column=['ap_hi', 'ap_lo'])"
      ],
      "metadata": {
        "id": "l3xkRLK5wSho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be observed, the outliers for the 'height' and 'weight' features are removed. Let us also make sure that the systolic pressure 'ap_hi' is always higher than the diastolic pressure 'ap_lo'."
      ],
      "metadata": {
        "id": "p3GLdpeK1REj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Systolic pressure is higher than diastolic pressure in {0}% of the patient records\".format(100*df[df['ap_hi']> df['ap_lo']].shape[0]/df.shape[0]))"
      ],
      "metadata": {
        "id": "DrSfsZtB1PJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remove Outliers: the other features**\n",
        "\n",
        "The values of the other features are limited within a small range as can be observed from the min and max values in the statistical description table. Let us check if these features take only discrete values."
      ],
      "metadata": {
        "id": "zMr1aNHm4nnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('The discrete values of the \\'cholesterol\\' feature are {}'.format(set(df['cholesterol'].unique())))\n",
        "print('The discrete values of the \\'gluc\\' feature are {}'.format(set(df['gluc'].unique())))\n",
        "print('The discrete values of the \\'active\\' feature are {}'.format(set(df['active'].unique())))"
      ],
      "metadata": {
        "id": "TQIsmK525qF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the range of the other features is limited and the values are discrete so no need to apply outliers removal techniques for these features."
      ],
      "metadata": {
        "id": "EQwnJRgA7LAH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Data"
      ],
      "metadata": {
        "id": "-SEz47bidar5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will save the clean dataset into a CSV file to be used in the next session."
      ],
      "metadata": {
        "id": "boXoDJ1hdfQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"/content/AIData/cardio_train_cleaned.csv\",sep=\";\",index=False)"
      ],
      "metadata": {
        "id": "sgb3CQnGd9il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the '/content/AIData/' folder for the 'cardio_train_cleaned.csv' file and download it for future usage."
      ],
      "metadata": {
        "id": "itbI0J9ZeKPB"
      }
    }
  ]
}